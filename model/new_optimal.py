"""
ê°œì„ ëœ LSì „ê¸° ì „ê¸°ìš”ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸
- ëª¨ë“ˆí™”ëœ ì½”ë“œ êµ¬ì¡°
- ê³ ê¸‰ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ë¶„ ë‹¨ìœ„ í¬í•¨)
- ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ë§ ì „ëµ
- ì‹œê³„ì—´ êµì°¨ê²€ì¦
- ìŠ¤íƒœí‚¹ ì•™ìƒë¸”
- LSTM ì•„í‚¤í…ì²˜ ê°œì„ 
- ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ
- ëª¨ë¸ í•´ì„ ë° ì‹¤í—˜ ì¶”ì 
"""

import os
import pickle
import warnings
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass

import numpy as np
import pandas as pd
import optuna
import shap
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler
from sklearn.model_selection import TimeSeriesSplit, cross_val_predict
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.linear_model import LinearRegression
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

try:
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    STATS_AVAILABLE = True
except ImportError:
    STATS_AVAILABLE = False

warnings.filterwarnings('ignore')

@dataclass
class Config:
    """ì„¤ì • í´ë˜ìŠ¤"""
    BASE_DIR: str = "../dashboard/data"
    MODELS_DIR: str = "pickles"
    TIME_STEPS: int = 96 * 7  # 7ì¼ ì‹œí€€ìŠ¤
    TEST_SIZE: float = 0.2
    RANDOM_STATE: int = 42
    N_TRIALS: int = 50
    CV_FOLDS: int = 5
    
    # íƒ€ê²Ÿ ë³€ìˆ˜ë“¤
    MULTI_TARGETS: List[str] = None
    
    def __post_init__(self):
        if self.MULTI_TARGETS is None:
            self.MULTI_TARGETS = [
                "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
                "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)", 
                "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
                "íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)",
                "ì§€ìƒì—­ë¥ (%)",
                "ì§„ìƒì—­ë¥ (%)",
                "ì „ê¸°ìš”ê¸ˆ(ì›)"
            ]

class DataProcessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        self.config = config
        self.le = LabelEncoder()
        self.scalers = {}
        self.isolation_forest = IsolationForest(contamination=0.1, random_state=config.RANDOM_STATE)
        
        # ìš”ê¸ˆ í…Œì´ë¸”
        self.RATE_TABLE = {
            "before": {
                "ì—¬ë¦„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
                "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
                "ê²¨ìš¸": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4},
            },
            "after": {
                "ì—¬ë¦„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
                "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
                "ê²¨ìš¸": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3},
            },
        }
        self.CUTOFF = datetime(2024, 10, 24)
    
    def load_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """ë°ì´í„° ë¡œë“œ"""
        train_df = pd.read_csv(os.path.join(self.config.BASE_DIR, "train.csv"))
        test_df = pd.read_csv(os.path.join(self.config.BASE_DIR, "test.csv"))
        return train_df, test_df
    
    def create_datetime_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ ì‹œê°„ í”¼ì²˜ ìƒì„± (ë¶„ ë‹¨ìœ„ í¬í•¨)"""
        df = df.copy()
        df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"])
        
        # ê¸°ë³¸ ì‹œê°„ í”¼ì²˜
        df["ë…„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.year
        df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
        df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
        df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
        df["ë¶„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.minute  # ë¶„ ì¶”ê°€
        df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
        df["ì£¼ë§ì—¬ë¶€"] = (df["ìš”ì¼"] >= 5).astype(int)
        
        # ìˆœí™˜ ì¸ì½”ë”©
        df["sin_ì‹œê°„"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
        df["cos_ì‹œê°„"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)
        df["sin_ë¶„"] = np.sin(2 * np.pi * df["ë¶„"] / 60)  # ë¶„ ìˆœí™˜ ì¸ì½”ë”©
        df["cos_ë¶„"] = np.cos(2 * np.pi * df["ë¶„"] / 60)
        df["sin_ì›”"] = np.sin(2 * np.pi * df["ì›”"] / 12)
        df["cos_ì›”"] = np.cos(2 * np.pi * df["ì›”"] / 12)
        df["sin_ì¼"] = np.sin(2 * np.pi * df["ì¼"] / 31)
        df["cos_ì¼"] = np.cos(2 * np.pi * df["ì¼"] / 31)
        df["sin_ìš”ì¼"] = np.sin(2 * np.pi * df["ìš”ì¼"] / 7)
        df["cos_ìš”ì¼"] = np.cos(2 * np.pi * df["ìš”ì¼"] / 7)
        
        # ê³„ì ˆ í”¼ì²˜
        df["ê³„ì ˆ_encoded"] = df["ì›”"].apply(lambda x: (x-1)//3)
        
        # ì‹œê°„ëŒ€ ë¶„ë¥˜ (ë” ì„¸ë¶„í™”)
        df["ì‹œê°„ëŒ€_ì„¸ë¶„í™”"] = df["ì‹œê°„"].apply(self._get_detailed_time_period)
        
        return df
    
    def _get_detailed_time_period(self, hour: int) -> str:
        """ì„¸ë¶„í™”ëœ ì‹œê°„ëŒ€ ë¶„ë¥˜"""
        if 6 <= hour < 9:
            return "ì¶œê·¼ì‹œê°„"
        elif 9 <= hour < 12:
            return "ì˜¤ì „ì—…ë¬´"
        elif 12 <= hour < 14:
            return "ì ì‹¬ì‹œê°„"
        elif 14 <= hour < 18:
            return "ì˜¤í›„ì—…ë¬´"
        elif 18 <= hour < 21:
            return "í‡´ê·¼ì‹œê°„"
        elif 21 <= hour < 24:
            return "ì €ë…ì‹œê°„"
        else:
            return "ì•¼ê°„ì‹œê°„"
    
    def get_season(self, month: int) -> str:
        """ê³„ì ˆ ë¶„ë¥˜"""
        if month in [6, 7, 8]:
            return "ì—¬ë¦„"
        elif month in [3, 4, 5, 9, 10]:
            return "ë´„ê°€ì„"
        return "ê²¨ìš¸"
    
    def get_time_zone(self, hour: int, season: str) -> str:
        """ì‹œê°„ëŒ€ë³„ ì „ë ¥ ë¶€í•˜ ë¶„ë¥˜"""
        if season in ["ì—¬ë¦„", "ë´„ê°€ì„"]:
            if 22 <= hour or hour < 8:
                return "ê²½ë¶€í•˜"
            if (8 <= hour < 11) or (12 <= hour < 13) or (18 <= hour < 22):
                return "ì¤‘ê°„ë¶€í•˜"
            return "ìµœëŒ€ë¶€í•˜"
        else:
            if 22 <= hour or hour < 8:
                return "ê²½ë¶€í•˜"
            if (8 <= hour < 9) or (12 <= hour < 16) or (19 <= hour < 22):
                return "ì¤‘ê°„ë¶€í•˜"
            return "ìµœëŒ€ë¶€í•˜"
    
    def create_tariff_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ìš”ê¸ˆ ê´€ë ¨ í”¼ì²˜ ìƒì„±"""
        df = df.copy()
        df["ê³„ì ˆ"] = df["ì›”"].apply(self.get_season)
        df["ì ìš©ì‹œì "] = df["ì¸¡ì •ì¼ì‹œ"].apply(lambda x: "before" if x < self.CUTOFF else "after")
        df["ì‹œê°„ëŒ€"] = df.apply(lambda r: self.get_time_zone(r["ì‹œê°„"], r["ê³„ì ˆ"]), axis=1)
        df["ìš”ê¸ˆë‹¨ê°€"] = df.apply(lambda r: self.RATE_TABLE[r["ì ìš©ì‹œì "]][r["ê³„ì ˆ"]][r["ì‹œê°„ëŒ€"]], axis=1)
        return df
    
    def create_lag_features(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """ì§€ì—° í”¼ì²˜ ìƒì„±"""
        df = df.copy()
        df = df.sort_values('ì¸¡ì •ì¼ì‹œ').reset_index(drop=True)
        
        # ë‹¤ì–‘í•œ ì§€ì—° ê¸°ê°„
        lags = [1, 2, 6, 12, 24, 48, 96, 168, 336]  # 15ë¶„ë¶€í„° 2ì£¼ê¹Œì§€
        
        for lag in lags:
            df[f"{target_col}_lag_{lag}"] = df[target_col].shift(lag)
        
        return df
    
    def create_rolling_features(self, df: pd.DataFrame, target_col: str) -> pd.DataFrame:
        """ë¡¤ë§ í†µê³„ í”¼ì²˜ ìƒì„±"""
        df = df.copy()
        df = df.sort_values('ì¸¡ì •ì¼ì‹œ').reset_index(drop=True)
        
        # ë‹¤ì–‘í•œ ìœˆë„ìš° í¬ê¸°
        windows = [6, 12, 24, 48, 96, 168]  # 1.5ì‹œê°„ë¶€í„° 1ì£¼ê¹Œì§€
        
        for window in windows:
            df[f"{target_col}_rolling_mean_{window}"] = df[target_col].rolling(window).mean()
            df[f"{target_col}_rolling_std_{window}"] = df[target_col].rolling(window).std()
            df[f"{target_col}_rolling_max_{window}"] = df[target_col].rolling(window).max()
            df[f"{target_col}_rolling_min_{window}"] = df[target_col].rolling(window).min()
            df[f"{target_col}_rolling_median_{window}"] = df[target_col].rolling(window).median()
            
            # ë³€í™”ìœ¨ í”¼ì²˜
            df[f"{target_col}_pct_change_{window}"] = df[target_col].pct_change(window)
        
        return df
    
    def target_encoding(self, df_train: pd.DataFrame, df_test: pd.DataFrame, 
                       col: str, target: str, smoothing: int = 10) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """íƒ€ê²Ÿ ì¸ì½”ë”©"""
        global_mean = df_train[target].mean()
        agg = df_train.groupby(col)[target].agg(["mean", "count"])
        smoothing_weight = 1 / (1 + np.exp(-(agg["count"] - smoothing)))
        enc = global_mean * (1 - smoothing_weight) + agg["mean"] * smoothing_weight
        mapping = enc.to_dict()
        
        df_train = df_train.copy()
        df_test = df_test.copy()
        df_train[f"{col}_te"] = df_train[col].map(mapping)
        df_test[f"{col}_te"] = df_test[col].map(mapping)
        
        return df_train, df_test
    
    def remove_outliers(self, df: pd.DataFrame, target_col: str, method: str = "combined") -> pd.DataFrame:
        """ê°œì„ ëœ ì´ìƒì¹˜ ì œê±°"""
        df = df.copy()
        
        if method == "iqr":
            q1 = df[target_col].quantile(0.25)
            q3 = df[target_col].quantile(0.75)
            iqr = q3 - q1
            lower = q1 - 1.5 * iqr
            upper = q3 + 1.5 * iqr
            mask = (df[target_col] >= lower) & (df[target_col] <= upper)
            
        elif method == "isolation_forest":
            features_for_outlier = [col for col in df.columns if col != target_col and df[col].dtype in ['int64', 'float64']]
            if features_for_outlier:
                outlier_pred = self.isolation_forest.fit_predict(df[features_for_outlier].fillna(0))
                mask = outlier_pred == 1
            else:
                mask = pd.Series([True] * len(df))
                
        elif method == "combined":
            # IQR + Isolation Forest ì¡°í•©
            q1 = df[target_col].quantile(0.25)
            q3 = df[target_col].quantile(0.75)
            iqr = q3 - q1
            lower = q1 - 1.5 * iqr
            upper = q3 + 1.5 * iqr
            iqr_mask = (df[target_col] >= lower) & (df[target_col] <= upper)
            
            features_for_outlier = [col for col in df.columns if col != target_col and df[col].dtype in ['int64', 'float64']]
            if features_for_outlier:
                outlier_pred = self.isolation_forest.fit_predict(df[features_for_outlier].fillna(0))
                iso_mask = outlier_pred == 1
                mask = iqr_mask & iso_mask
            else:
                mask = iqr_mask
        
        return df[mask]
    
    def get_feature_groups(self) -> Dict[str, List[str]]:
        """í”¼ì²˜ ê·¸ë£¹ ì •ì˜ (ë¶„ í¬í•¨)"""
        return {
            'numerical_features': [
                'ìš”ê¸ˆë‹¨ê°€', 'ì‘ì—…ìœ í˜•_te', 'ì‹œê°„_te', 'ìš”ì¼_te', 'ì‹œê°„ëŒ€_te',
                'sin_ì‹œê°„', 'cos_ì‹œê°„', 'sin_ë¶„', 'cos_ë¶„', 'sin_ì›”', 'cos_ì›”', 
                'sin_ì¼', 'cos_ì¼', 'sin_ìš”ì¼', 'cos_ìš”ì¼'
            ],
            'categorical_features': [
                'ì‘ì—…ìœ í˜•_encoded', 'ë…„', 'ì›”', 'ì¼', 'ì‹œê°„', 'ë¶„', 'ìš”ì¼', 'ì£¼ë§ì—¬ë¶€',
                'ê³„ì ˆ_encoded', 'ì‹œê°„ëŒ€_ì„¸ë¶„í™”'
            ],
            'lag_features': [],  # ë™ì ìœ¼ë¡œ ì¶”ê°€ë¨
            'rolling_features': []  # ë™ì ìœ¼ë¡œ ì¶”ê°€ë¨
        }
    
    def scale_features(self, X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """í”¼ì²˜ íƒ€ì…ë³„ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ì ìš©"""
        # âœ… NaN / Inf ì œê±°
        X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)
        X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)

        feature_groups = self.get_feature_groups()
        actual_features = X_train.columns.tolist()

        # ë™ì ìœ¼ë¡œ lag, rolling í”¼ì²˜ ì°¾ê¸°
        for col in actual_features:
            if '_lag_' in col:
                feature_groups['lag_features'].append(col)
            elif '_rolling_' in col or '_pct_change_' in col:
                feature_groups['rolling_features'].append(col)

        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í”¼ì²˜ë§Œ í•„í„°ë§
        for group in feature_groups:
            feature_groups[group] = [f for f in feature_groups[group] if f in actual_features]

        # â— object íƒ€ì… ì œê±° (ìŠ¤ì¼€ì¼ëŸ¬ ì—ëŸ¬ ë°©ì§€)
        for group in ['categorical_features', 'numerical_features']:
            feature_groups[group] = [col for col in feature_groups[group] if X_train[col].dtype != 'object']

        scaled_train_parts = []
        scaled_test_parts = []
        feature_order = []

        # ìˆ˜ì¹˜í˜• - RobustScaler
        if feature_groups['numerical_features']:
            self.scalers['numerical'] = RobustScaler()
            num_train = self.scalers['numerical'].fit_transform(X_train[feature_groups['numerical_features']])
            num_test = self.scalers['numerical'].transform(X_test[feature_groups['numerical_features']])
            scaled_train_parts.append(num_train)
            scaled_test_parts.append(num_test)
            feature_order.extend(feature_groups['numerical_features'])

        # ë²”ì£¼í˜• - StandardScaler
        if feature_groups['categorical_features']:
            self.scalers['categorical'] = StandardScaler()
            cat_train = self.scalers['categorical'].fit_transform(X_train[feature_groups['categorical_features']])
            cat_test = self.scalers['categorical'].transform(X_test[feature_groups['categorical_features']])
            scaled_train_parts.append(cat_train)
            scaled_test_parts.append(cat_test)
            feature_order.extend(feature_groups['categorical_features'])

        # Lag í”¼ì²˜ - MinMaxScaler
        if feature_groups['lag_features']:
            self.scalers['lag'] = MinMaxScaler()
            lag_train = self.scalers['lag'].fit_transform(X_train[feature_groups['lag_features']])
            lag_test = self.scalers['lag'].transform(X_test[feature_groups['lag_features']])
            scaled_train_parts.append(lag_train)
            scaled_test_parts.append(lag_test)
            feature_order.extend(feature_groups['lag_features'])

        # Rolling í”¼ì²˜ - RobustScaler
        if feature_groups['rolling_features']:
            self.scalers['rolling'] = RobustScaler()
            roll_train = self.scalers['rolling'].fit_transform(X_train[feature_groups['rolling_features']])
            roll_test = self.scalers['rolling'].transform(X_test[feature_groups['rolling_features']])
            scaled_train_parts.append(roll_train)
            scaled_test_parts.append(roll_test)
            feature_order.extend(feature_groups['rolling_features'])

        # ë³‘í•©
        X_train_scaled = np.hstack(scaled_train_parts) if scaled_train_parts else X_train.values
        X_test_scaled = np.hstack(scaled_test_parts) if scaled_test_parts else X_test.values

        self.feature_order = feature_order
        return X_train_scaled, X_test_scaled

class ModelEnsemble:
    """ì•™ìƒë¸” ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(self, config: Config):
        self.config = config
        self.base_models = {}
        self.meta_model = LinearRegression()
        self.lstm_model = None
        self.sarimax_model = None
        self.scalers = {}

    def create_base_models(self) -> Dict[str, Any]:
        """ë² ì´ìŠ¤ ëª¨ë¸ë“¤ ìƒì„±"""
        return {
            "xgb": XGBRegressor(
                n_estimators=500, max_depth=6, learning_rate=0.02,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.config.RANDOM_STATE, n_jobs=-1
            ),
            "lgb": LGBMRegressor(
                n_estimators=500, max_depth=6, learning_rate=0.02,
                subsample=0.8, colsample_bytree=0.8,
                random_state=self.config.RANDOM_STATE, n_jobs=-1, verbose=-1
            ),
            "rf": RandomForestRegressor(
                n_estimators=400, max_depth=12, min_samples_split=5,
                random_state=self.config.RANDOM_STATE, n_jobs=-1
            )
        }

    def create_advanced_lstm(self, input_shape: Tuple[int, int], best_params: Dict) -> Model:
        """ê°œì„ ëœ LSTM ì•„í‚¤í…ì²˜"""
        model = Sequential([
            LSTM(best_params["units1"], return_sequences=True, input_shape=input_shape),
            BatchNormalization(),
            Dropout(best_params["dropout"]),
            LSTM(best_params["units2"], return_sequences=True),
            BatchNormalization(),
            Dropout(best_params["dropout"]),
            LSTM(best_params["units3"]),
            BatchNormalization(),
            Dropout(best_params["dropout"]),
            Dense(64, activation="relu"),
            BatchNormalization(),
            Dropout(0.2),
            Dense(32, activation="relu"),
            Dense(1)
        ])
        model.compile(optimizer=Adam(learning_rate=best_params["lr"]),
                      loss=self.custom_loss, metrics=['mae'])
        return model

    def custom_loss(self, y_true, y_pred):
        mse = tf.keras.losses.mse(y_true, y_pred)
        mae = tf.keras.losses.mae(y_true, y_pred)
        return 0.7 * mse + 0.3 * mae

    def optimize_lstm_hyperparameters(self, X_seq_train, y_seq_train, X_seq_val, y_seq_val) -> Dict:
        def objective(trial):
            units1 = trial.suggest_int("units1", 64, 256)
            units2 = trial.suggest_int("units2", 32, 128)
            units3 = trial.suggest_int("units3", 16, 64)
            dropout = trial.suggest_float("dropout", 0.1, 0.5)
            lr = trial.suggest_loguniform("lr", 1e-5, 1e-2)

            model = Sequential([
                LSTM(units1, return_sequences=True, input_shape=(X_seq_train.shape[1], X_seq_train.shape[2])),
                Dropout(dropout),
                LSTM(units2, return_sequences=True),
                Dropout(dropout),
                LSTM(units3),
                Dropout(dropout),
                Dense(32, activation="relu"),
                Dense(1)
            ])

            model.compile(optimizer=Adam(learning_rate=lr), loss="mse")
            es = EarlyStopping(patience=3, restore_best_weights=True, verbose=0)

            model.fit(X_seq_train, y_seq_train,
                      validation_data=(X_seq_val, y_seq_val),
                      epochs=30, batch_size=64,
                      callbacks=[es], verbose=0)

            val_pred = model.predict(X_seq_val, verbose=0)
            return mean_absolute_error(y_seq_val, val_pred)

        study = optuna.create_study(direction="minimize")
        study.optimize(objective, n_trials=self.config.N_TRIALS)
        return study.best_params

    def create_stacking_ensemble(self, X_train, y_train, X_val, y_val):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ìƒì„±"""
        from sklearn.base import clone

        self.base_models = self.create_base_models()
        meta_features_train = np.zeros((len(X_train), len(self.base_models)))
        meta_features_val = np.zeros((len(X_val), len(self.base_models)))
        tscv = TimeSeriesSplit(n_splits=self.config.CV_FOLDS)

        for i, (name, model) in enumerate(self.base_models.items()):
            print(f"ğŸ”§ Training base model: {name}")
            oof_preds = np.zeros(len(X_train))

            for train_idx, val_idx in tscv.split(X_train):
                X_tr, X_val_fold = X_train[train_idx], X_train[val_idx]
                y_tr, y_val_fold = y_train[train_idx], y_train[val_idx]

                cloned_model = clone(model)
                cloned_model.fit(X_tr, y_tr)
                oof_preds[val_idx] = cloned_model.predict(X_val_fold)

            meta_features_train[:, i] = oof_preds

            # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ìš© ì „ì²´ ì¬í•™ìŠµ
            model.fit(X_train, y_train)
            meta_features_val[:, i] = model.predict(X_val)

        # ë©”íƒ€ ëª¨ë¸ í›ˆë ¨
        print("ğŸ“ˆ Training meta model (LinearRegression)")
        self.meta_model.fit(meta_features_train, y_train)

        return meta_features_val

    def predict_stacking(self, X_test):
        """ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì˜ˆì¸¡"""
        meta_features_test = np.zeros((len(X_test), len(self.base_models)))
        for i, (name, model) in enumerate(self.base_models.items()):
            meta_features_test[:, i] = model.predict(X_test)

        return self.meta_model.predict(meta_features_test)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam

class MultiTaskModel:
    """ë©€í‹°íƒœìŠ¤í¬ í•™ìŠµ ëª¨ë¸ í´ë˜ìŠ¤"""

    def __init__(self, config: Config):
        self.config = config
        self.model = None
        self.scalers = {}

    def create_multitask_model(self, input_shape: int, n_targets: int) -> Model:
        """ê³µí†µ íŠ¹ì„± ì¶”ì¶œê¸° + ê° íƒ€ê²Ÿë³„ í—¤ë“œë¥¼ ê°–ëŠ” ë©€í‹°íƒœìŠ¤í¬ ì‹ ê²½ë§ ìƒì„±"""
        
        input_layer = Input(shape=(input_shape,), name='input_layer')

        # ğŸ§± Shared layers
        x = Dense(256, activation='relu')(input_layer)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)

        x = Dense(128, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.3)(x)

        x = Dense(64, activation='relu')(x)
        x = BatchNormalization()(x)
        x = Dropout(0.2)(x)

        # ğŸ¯ Task-specific heads
        outputs = []
        for i in range(n_targets):
            head = Dense(32, activation='relu')(x)
            head = Dense(16, activation='relu')(head)
            head = Dense(1, name=f'target_{i}')(head)
            outputs.append(head)

        # ğŸ§  Model compile
        self.model = Model(inputs=input_layer, outputs=outputs)
        self.model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        return self.model


class ElectricityPredictor:
    """LS ì „ê¸°ìš”ê¸ˆ ì˜ˆì¸¡ ì „ì²´ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config=None):
        self.config = config or Config()
        self.data_processor = DataProcessor(self.config)
        self.model_ensemble = ModelEnsemble(self.config)
        self.multitask_model = MultiTaskModel(self.config)
        self.results = {}
        self.feature_importance = {}

        os.makedirs(self.config.MODELS_DIR, exist_ok=True)

    def time_based_split(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
        df_sorted = df.sort_values('ì¸¡ì •ì¼ì‹œ').reset_index(drop=True)
        split_point = int(len(df_sorted) * (1 - self.config.TEST_SIZE))
        return df_sorted.iloc[:split_point].copy(), df_sorted.iloc[split_point:].copy()

    def prepare_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        train_df, test_df = self.data_processor.load_data()
        train_df = self.data_processor.create_datetime_features(train_df)
        test_df = self.data_processor.create_datetime_features(test_df)
        train_df = self.data_processor.create_tariff_features(train_df)
        test_df = self.data_processor.create_tariff_features(test_df)

        target_col = "ì „ê¸°ìš”ê¸ˆ(ì›)"
        train_df = self.data_processor.create_lag_features(train_df, target_col)
        train_df = self.data_processor.create_rolling_features(train_df, target_col)

        for col in train_df.columns:
            if col not in test_df.columns and ('_lag_' in col or '_rolling_' in col or '_pct_change_' in col):
                test_df[col] = np.nan

        train_df = self.data_processor.remove_outliers(train_df, target_col)

        for col in ['ì‘ì—…ìœ í˜•', 'ì‹œê°„', 'ìš”ì¼', 'ì‹œê°„ëŒ€']:
            train_df, test_df = self.data_processor.target_encoding(train_df, test_df, col, target=target_col)

        for col in ['ì‘ì—…ìœ í˜•', 'ì‹œê°„ëŒ€_ì„¸ë¶„í™”']:
            le = LabelEncoder()
            train_df[col + "_encoded"] = le.fit_transform(train_df[col])
            test_df[col + "_encoded"] = le.transform(test_df[col])

        return train_df, test_df

    def run(self):
        train_df, test_df = self.prepare_data()
        train_split, val_split = self.time_based_split(train_df)
        target_col = "ì „ê¸°ìš”ê¸ˆ(ì›)"

        X_train = train_split.drop(columns=self.config.MULTI_TARGETS + ['ì¸¡ì •ì¼ì‹œ'])
        y_train = train_split[target_col]
        X_val = val_split.drop(columns=self.config.MULTI_TARGETS + ['ì¸¡ì •ì¼ì‹œ'])
        y_val = val_split[target_col]
        X_test = test_df.drop(columns=['ì¸¡ì •ì¼ì‹œ'])

        X_train_scaled, X_val_scaled = self.data_processor.scale_features(X_train, X_val)
        _, X_test_scaled = self.data_processor.scale_features(X_train, X_test)

        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=1e10, neginf=-1e10)
        X_val_scaled = np.nan_to_num(X_val_scaled, nan=0.0, posinf=1e10, neginf=-1e10)
        X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1e10, neginf=-1e10)

        target_scaler = StandardScaler()
        y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).ravel()
        y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).ravel()

        self.model_ensemble.create_stacking_ensemble(X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled)

        val_preds_scaled = self.model_ensemble.predict_stacking(X_val_scaled)
        val_preds = target_scaler.inverse_transform(val_preds_scaled.reshape(-1, 1)).ravel()

        test_preds_scaled = self.model_ensemble.predict_stacking(X_test_scaled)
        test_preds = target_scaler.inverse_transform(test_preds_scaled.reshape(-1, 1)).ravel()
        test_preds = np.clip(test_preds, 0, None)

        print("ğŸ“Š MAE: {:.4f}, RMSE: {:.4f}, R2: {:.4f}".format(
            mean_absolute_error(y_val, val_preds),
            np.sqrt(mean_squared_error(y_val, val_preds)),
            r2_score(y_val, val_preds)
        ))

        submission_df = pd.DataFrame({
            'id': test_df['id'],
            'ì˜ˆì¸¡ ì „ê¸°ìš”ê¸ˆ(ì›)': test_preds
        })

        save_path = os.path.join(self.config.MODELS_DIR, "test_predictions.csv")
        submission_df.to_csv(save_path, index=False, encoding='utf-8-sig')
        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {save_path}")

if __name__ == "__main__":
    predictor = ElectricityPredictor()
    predictor.run()
