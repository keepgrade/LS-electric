import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import r2_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# 1. ë°ì´í„° ë¡œë”©
train_df = pd.read_csv("../data/train.csv")

# 2. íŒŒìƒ ë³€ìˆ˜ ìƒì„± (ì‹œê°„ ê´€ë ¨)
train_df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(train_df["ì¸¡ì •ì¼ì‹œ"])
train_df["ì›”"] = train_df["ì¸¡ì •ì¼ì‹œ"].dt.month
train_df["ì¼"] = train_df["ì¸¡ì •ì¼ì‹œ"].dt.day
train_df["ì‹œê°„"] = train_df["ì¸¡ì •ì¼ì‹œ"].dt.hour
train_df["ìš”ì¼"] = train_df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
train_df["sin_ì‹œê°„"] = np.sin(2 * np.pi * train_df["ì‹œê°„"] / 24)
train_df["cos_ì‹œê°„"] = np.cos(2 * np.pi * train_df["ì‹œê°„"] / 24)

# 3. ì‚¬ìš©í•  í”¼ì²˜ ë° íƒ€ê²Ÿ ì •ì˜
FEATURES = ["ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "sin_ì‹œê°„", "cos_ì‹œê°„"]
TARGET = "ì „ê¸°ìš”ê¸ˆ(ì›)"

# 4. ìŠ¤ì¼€ì¼ë§
scaler = MinMaxScaler()
train_scaled = pd.DataFrame(scaler.fit_transform(train_df[FEATURES + [TARGET]]), columns=FEATURES + [TARGET])

# 5. ì‹œê³„ì—´ ì¦ê°• í•¨ìˆ˜
def create_augmented_sequences(data, features, target, time_steps=672, n_slices=2, noise_level=0.01):
    xs, ys = [], []
    for i in range(len(data) - time_steps - 1):
        base_seq = data.iloc[i:i+time_steps].copy()
        base_target = data.iloc[i+time_steps][target]
        xs.append(base_seq[features].values)
        ys.append(base_target)
        for _ in range(n_slices):
            sliced = base_seq.sample(frac=1).sort_index()
            noisy = sliced[features].values + np.random.normal(0, noise_level, sliced[features].shape)
            xs.append(noisy)
            ys.append(base_target)
    return np.array(xs), np.array(ys)

# 6. ì¦ê°• ì‹œí€€ìŠ¤ ìƒì„±
X_seq, y_seq = create_augmented_sequences(train_scaled, FEATURES, TARGET, time_steps=96*7, n_slices=2)

# 7. í›ˆë ¨/ê²€ì¦ ë¶„ë¦¬
split_idx = int(len(X_seq) * 0.8)
X_train, X_val = X_seq[:split_idx], X_seq[split_idx:]
y_train, y_val = y_seq[:split_idx], y_seq[split_idx:]

# 8. LSTM ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ
model = Sequential([
    LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])),
    Dense(32, activation="relu"),
    Dense(1)
])
model.compile(optimizer="adam", loss="mse")

es = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks=[es])

# 9. ê²€ì¦ ì„±ëŠ¥
val_pred = model.predict(X_val).flatten()
print("ğŸ“Š LSTM with Augmentation RÂ²:", r2_score(y_val, val_pred))






# 10. test.csv ë¶ˆëŸ¬ì˜¤ê¸° ë° í”¼ì²˜ ìƒì„±
test_df = pd.read_csv("../data/test.csv")
test_df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(test_df["ì¸¡ì •ì¼ì‹œ"])
test_df["ì›”"] = test_df["ì¸¡ì •ì¼ì‹œ"].dt.month
test_df["ì¼"] = test_df["ì¸¡ì •ì¼ì‹œ"].dt.day
test_df["ì‹œê°„"] = test_df["ì¸¡ì •ì¼ì‹œ"].dt.hour
test_df["ìš”ì¼"] = test_df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
test_df["sin_ì‹œê°„"] = np.sin(2 * np.pi * test_df["ì‹œê°„"] / 24)
test_df["cos_ì‹œê°„"] = np.cos(2 * np.pi * test_df["ì‹œê°„"] / 24)


FEATURES = ["ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "sin_ì‹œê°„", "cos_ì‹œê°„"]


# í•™ìŠµ ì‹œì ì— ì „ê¸°ìš”ê¸ˆ(ì›)ì€ ì œì™¸
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(train_df[FEATURES])  # TARGETì€ ì œì™¸
joblib.dump(scaler, "models/minmax_scaler.pkl")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ê°™ì€ FEATURESë¡œë§Œ transform
test_scaled = pd.DataFrame(scaler.transform(test_df[FEATURES]), columns=FEATURES)

# ìŠ¤ì¼€ì¼ë§
test_scaled = pd.DataFrame(scaler.transform(test_df[FEATURES]), columns=FEATURES)



# LSTM ì…ë ¥ ì‹œí€€ìŠ¤ êµ¬ì„±
def create_lstm_sequences(test_features_df, last_known, time_steps=96*7):
    combined = pd.concat([last_known, test_features_df], ignore_index=True)
    seqs = []
    for i in range(len(test_features_df)):
        start_idx = i
        end_idx = i + time_steps
        seq = combined.iloc[start_idx:end_idx][FEATURES].values
        seqs.append(seq)
    return np.array(seqs[-len(test_features_df):])  # ê°€ì¥ ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€

# ìµœê·¼ í•™ìŠµ ë°ì´í„°ì—ì„œ ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ê°€ì ¸ì˜¤ê¸°
last_known_scaled = train_scaled[FEATURES].iloc[-(96*7):]

# ì‹œí€€ìŠ¤ ìƒì„±
X_test_seq = create_lstm_sequences(test_scaled, last_known_scaled)

# ì˜ˆì¸¡
test_pred = model.predict(X_test_seq).flatten()

# submission ì €ì¥
submission = pd.DataFrame({
    "id": test_df["id"],
    "ì „ê¸°ìš”ê¸ˆ(ì›)": test_pred
})
submission.to_csv("submission.csv", index=False)
print("âœ… submission.csv ì €ì¥ ì™„ë£Œ")

# ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
os.makedirs("models", exist_ok=True)
model.save("models/lstm_model.h5")
joblib.dump(scaler, "models/minmax_scaler.pkl")
print("âœ… ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ ì™„ë£Œ")
