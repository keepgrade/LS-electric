"""Optimal model for LS electric electricity bill prediction.

This script combines advanced preprocessing steps with multiple models:
- LSTM on 96x7 time windows
- Tree based models (XGB, LightGBM, RandomForest)
- Optional SARIMAX if statsmodels is installed

The final prediction is an ensemble of the available models.
"""

import os
import pickle
from datetime import datetime
import optuna

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import tensorflow as tf

try:
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    STATS_AVAILABLE = True
except Exception:
    STATS_AVAILABLE = False

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dropout, BatchNormalization

# ----------------------------------------------------------------------
# 1. Load data
# ----------------------------------------------------------------------
BASE_DIR = "../dashboard/data"
train_df = pd.read_csv(os.path.join(BASE_DIR, "train.csv"))
test_df = pd.read_csv(os.path.join(BASE_DIR, "test.csv"))

# ----------------------------------------------------------------------
# 2. Datetime features
# ----------------------------------------------------------------------
for df in [train_df, test_df]:
    df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"])
    df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
    df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
    df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
    df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
    df["ì£¼ë§ì—¬ë¶€"] = (df["ìš”ì¼"] >= 5).astype(int)
    df["sin_ì‹œê°„"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
    df["cos_ì‹œê°„"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)

# ----------------------------------------------------------------------
# 3. Tariff calculation
# ----------------------------------------------------------------------
def get_season(month: int) -> str:
    if month in [6, 7, 8]:
        return "ì—¬ë¦„"
    elif month in [3, 4, 5, 9, 10]:
        return "ë´„ê°€ì„"
    return "ê²¨ìš¸"


def get_time_zone(hour: int, season: str) -> str:
    if season in ["ì—¬ë¦„", "ë´„ê°€ì„"]:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 11) or (12 <= hour < 13) or (18 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 9) or (12 <= hour < 16) or (19 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"


RATE_TABLE = {
    "before": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4},
    },
    "after": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3},
    },
}

CUTOFF = datetime(2024, 10, 24)
for df in [train_df, test_df]:
    df["ê³„ì ˆ"] = df["ì›”"].apply(get_season)
    df["ì ìš©ì‹œì "] = df["ì¸¡ì •ì¼ì‹œ"].apply(lambda x: "before" if x < CUTOFF else "after")
    df["ì‹œê°„ëŒ€"] = df.apply(lambda r: get_time_zone(r["ì‹œê°„"], r["ê³„ì ˆ"]), axis=1)
    df["ìš”ê¸ˆë‹¨ê°€"] = df.apply(lambda r: RATE_TABLE[r["ì ìš©ì‹œì "]][r["ê³„ì ˆ"]][r["ì‹œê°„ëŒ€"]], axis=1)

# ----------------------------------------------------------------------
# 4. Encoding and target encoding
# ----------------------------------------------------------------------
le = LabelEncoder()
train_df["ì‘ì—…ìœ í˜•_encoded"] = le.fit_transform(train_df["ì‘ì—…ìœ í˜•"])
test_df["ì‘ì—…ìœ í˜•_encoded"] = le.transform(test_df["ì‘ì—…ìœ í˜•"])


def target_encoding(df_train: pd.DataFrame, df_test: pd.DataFrame, col: str, target: str, smoothing: int = 10) -> None:
    global_mean = df_train[target].mean()
    agg = df_train.groupby(col)[target].agg(["mean", "count"])
    smoothing_weight = 1 / (1 + np.exp(-(agg["count"] - smoothing)))
    enc = global_mean * (1 - smoothing_weight) + agg["mean"] * smoothing_weight
    mapping = enc.to_dict()
    df_train[f"{col}_te"] = df_train[col].map(mapping)
    df_test[f"{col}_te"] = df_test[col].map(mapping)


for c in ["ì‘ì—…ìœ í˜•", "ì‹œê°„", "ìš”ì¼", "ì‹œê°„ëŒ€"]:
    target_encoding(train_df, test_df, c, "ì „ê¸°ìš”ê¸ˆ(ì›)")

# ----------------------------------------------------------------------
# 5. Outlier removal with IQR
# ----------------------------------------------------------------------
q1 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.25)
q3 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
train_df = train_df[(train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] >= lower) & (train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] <= upper)].copy()

# ----------------------------------------------------------------------
# 6. Feature selection
# ----------------------------------------------------------------------
FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded",
    "ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "ì£¼ë§ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„",
    "ìš”ê¸ˆë‹¨ê°€",
    "ì‘ì—…ìœ í˜•_te", "ì‹œê°„_te", "ìš”ì¼_te", "ì‹œê°„ëŒ€_te",
]
TARGET = "ì „ê¸°ìš”ê¸ˆ(ì›)"

X = train_df[FEATURES]
y = train_df[TARGET]
X_test = test_df[FEATURES]

# ----------------------------------------------------------------------
# 7. Scaling
# ----------------------------------------------------------------------
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

# ----------------------------------------------------------------------
# 8. Tree based models
# ----------------------------------------------------------------------
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

models = {
    "xgb": XGBRegressor(n_estimators=400, max_depth=5, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1),
    "lgb": LGBMRegressor(n_estimators=400, max_depth=5, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, verbose=-1),
    "rf": RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42, n_jobs=-1),
}

preds_val = {}
preds_test = {}
metrics = {}

print("Training tree models...")
n_models = len(models)
for idx, (name, model) in enumerate(models.items(), 1):
    print(f"[{idx}/{n_models}] {name} start")
    model.fit(X_train, y_train)
    val_pred = model.predict(X_val)
    preds_test[name] = model.predict(X_test_scaled)
    preds_val[name] = val_pred
    metrics[name] = r2_score(y_val, val_pred)
    print(f"[{idx}/{n_models}] {name} done")

# ----------------------------------------------------------------------
# 9. Optional SARIMAX
# ----------------------------------------------------------------------
if STATS_AVAILABLE:
    sarimax = SARIMAX(y, exog=X_scaled, order=(1,1,1), seasonal_order=(1,1,1,24))
    sarimax_fit = sarimax.fit(disp=False)
    val_pred = sarimax_fit.predict(start=len(y_train), end=len(y_train)+len(y_val)-1, exog=X_val)
    preds_val["sarimax"] = val_pred
    preds_test["sarimax"] = sarimax_fit.predict(start=len(X_scaled), end=len(X_scaled)+len(X_test_scaled)-1, exog=X_test_scaled)
    metrics["sarimax"] = r2_score(y_val, val_pred)
else:
    print("statsmodels not available - skipping SARIMAX")

# ----------------------------------------------------------------------
# 10. LSTM model
# ----------------------------------------------------------------------

# 1. ë°ì´í„° ì¤€ë¹„

TIME_STEPS = 96 * 7  # 7ì¼ ê°„ ì‹œí€€ìŠ¤

# train_df, test_dfëŠ” ì´ë¯¸ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤
seq_scaler = MinMaxScaler()
seq_data = train_df[FEATURES + [TARGET]].copy()
seq_scaled = seq_scaler.fit_transform(seq_data)
seq_scaled = pd.DataFrame(seq_scaled, columns=FEATURES + [TARGET])

def create_sequences(arr: pd.DataFrame, timesteps: int):
    xs, ys = [], []
    for i in range(len(arr) - timesteps):
        xs.append(arr.iloc[i:i+timesteps][FEATURES].values)
        ys.append(arr.iloc[i+timesteps][TARGET])
    return np.array(xs), np.array(ys)

X_seq, y_seq = create_sequences(seq_scaled, TIME_STEPS)
seq_train = int(len(X_seq) * 0.8)
X_seq_train, X_seq_val = X_seq[:seq_train], X_seq[seq_train:]
y_seq_train, y_seq_val = y_seq[:seq_train], y_seq[seq_train:]


# 2. Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

def objective(trial):
    units1 = trial.suggest_int("units1", 64, 256)
    units2 = trial.suggest_int("units2", 32, 128)
    dropout = trial.suggest_float("dropout", 0.1, 0.5)
    lr = trial.suggest_loguniform("lr", 1e-5, 1e-2)

    model = Sequential([
        LSTM(units1, return_sequences=True, input_shape=(TIME_STEPS, len(FEATURES))),
        Dropout(dropout),
        LSTM(units2),
        Dropout(dropout),
        Dense(32, activation="relu"),
        Dense(1)
    ])
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss="mse")
    es = EarlyStopping(patience=3, restore_best_weights=True, verbose=0)

    model.fit(X_seq_train, y_seq_train,
              validation_data=(X_seq_val, y_seq_val),
              epochs=20, batch_size=32,
              callbacks=[es], verbose=0)

    val_pred = model.predict(X_seq_val)
    return mean_absolute_error(y_seq_val, val_pred)

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=30)

best_params = study.best_params
print("ğŸ“Œ ìµœì  íŒŒë¼ë¯¸í„°:", best_params)


# 3. ìµœì¢… ëª¨ë¸ í•™ìŠµ

lstm_model = Sequential([
    LSTM(best_params["units1"], return_sequences=True, input_shape=(TIME_STEPS, len(FEATURES))),
    Dropout(best_params["dropout"]),
    LSTM(best_params["units2"]),
    Dropout(best_params["dropout"]),
    Dense(32, activation="relu"),
    BatchNormalization(),
    Dense(1)
])
lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params["lr"]), loss="mse")

es = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True, verbose=1)
print("ğŸ“ˆ LSTM ìµœì¢… ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
lstm_model.fit(
    X_seq_train, y_seq_train,
    validation_data=(X_seq_val, y_seq_val),
    epochs=20, batch_size=32,
    callbacks=[es], verbose=1,
)

# 4. ëª¨ë¸ í‰ê°€ ë° ì˜ˆì¸¡

val_pred = lstm_model.predict(X_seq_val).flatten()
print("ğŸ“Š RÂ²:", r2_score(y_seq_val, val_pred))
print("ğŸ“Š MAE:", mean_absolute_error(y_seq_val, val_pred))

def predict_lstm(model, last_known: pd.DataFrame, future: pd.DataFrame) -> np.ndarray:
    combined = pd.concat([last_known, future], ignore_index=True)
    combined_scaled = seq_scaler.transform(combined)
    combined_scaled = pd.DataFrame(combined_scaled, columns=FEATURES + [TARGET])
    seqs = [combined_scaled.iloc[i:i+TIME_STEPS][FEATURES].values for i in range(len(combined_scaled) - TIME_STEPS)]
    seqs = np.array(seqs)
    preds = model.predict(seqs).flatten()
    return preds[-len(future):]

last_part = train_df[FEATURES + [TARGET]].iloc[-TIME_STEPS:]
lstm_test_pred = predict_lstm(lstm_model, last_part, test_df[FEATURES])

# ğŸ“¦ ê²°ê³¼ ì €ì¥
preds_test["lstm"] = lstm_test_pred
preds_val["lstm"] = val_pred

# ----------------------------------------------------------------------
# 11. Weighted ensemble
# ----------------------------------------------------------------------
weights = {name: max(score, 0) for name, score in metrics.items()}
total = sum(weights.values())
if total == 0:
    weights = {name: 1/len(weights) for name in weights}
else:
    weights = {name: w/total for name, w in weights.items()}

# Determine correct validation target
is_seq = "lstm" in preds_val
y_true = y_seq_val if is_seq else y_val
target_len = len(y_true)

# Initialize
val_ens = np.zeros(target_len)

# Ensemble
for name, pred in preds_val.items():
    if len(pred) != target_len:
        print(f"âš ï¸ {name} prediction length mismatch: {len(pred)} vs {target_len}")
        continue
    val_ens += weights[name] * pred

# Evaluate
ens_r2 = r2_score(y_true, val_ens)
print("âœ… Ensemble R2:", round(ens_r2, 4))

# Test prediction
test_pred = np.zeros(len(test_df))
for name, pred in preds_test.items():
    test_pred += weights[name] * pred


submission = pd.DataFrame({"id": test_df["id"], "ì „ê¸°ìš”ê¸ˆ(ì›)": test_pred})
submission.to_csv("submission_optimal.csv", index=False)
submission.to_csv("submission.csv", index=False)
print("Saved submission_optimal.csv and submission.csv")

# ----------------------------------------------------------------------
# 12. Save trained models
# ----------------------------------------------------------------------
MODELS_DIR = "pickles"
os.makedirs(MODELS_DIR, exist_ok=True)
print("Saving trained models...")
for name, model in models.items():
    with open(os.path.join(MODELS_DIR, f"{name}.pkl"), "wb") as f:
        pickle.dump(model, f)
    print(f"Saved {name}.pkl")
if STATS_AVAILABLE:
    with open(os.path.join(MODELS_DIR, "sarimax.pkl"), "wb") as f:
        pickle.dump(sarimax_fit, f)
    print("Saved sarimax.pkl")
with open(os.path.join(MODELS_DIR, "lstm.pkl"), "wb") as f:
    pickle.dump(lstm_model, f)
print("Saved lstm.pkl")
with open(os.path.join(MODELS_DIR, "scaler.pkl"), "wb") as f:
    pickle.dump(scaler, f)
print("Saved scaler.pkl")
with open(os.path.join(MODELS_DIR, "seq_scaler.pkl"), "wb") as f:
    pickle.dump(seq_scaler, f)
print("Saved seq_scaler.pkl")
print(f"Saved trained models to {MODELS_DIR}")




MULTI_TARGETS = [
    "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
    "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)",
    "ì§€ìƒì—­ë¥ (%)",
    "ì§„ìƒì—­ë¥ (%)",
    "ì „ê¸°ìš”ê¸ˆ(ì›)"
]

# test_df ìƒì„± ë° íŒŒìƒ ë³€ìˆ˜ í¬í•¨ ì™„ë£Œëœ ìƒíƒœë¼ê³  ê°€ì •
test_pred_dict = {}

for target in MULTI_TARGETS:
    print(f"ğŸ” [{target}] ì˜ˆì¸¡ ì¤‘...")
    
    # í•™ìŠµìš© ë°ì´í„° êµ¬ì„±
    y_multi = train_df[target]
    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_multi, test_size=0.2, random_state=42)

    # ëª¨ë¸ í›ˆë ¨ (ì—¬ê¸°ì„  LightGBM ì‚¬ìš© ì˜ˆì‹œ)
    model = LGBMRegressor(n_estimators=300, random_state=42)
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    y_test_pred = model.predict(X_test_scaled)
    test_pred_dict[target] = y_test_pred
    print(f"âœ… [{target}] ì˜ˆì¸¡ ì™„ë£Œ")

for target, pred in test_pred_dict.items():
    test_df[target] = pred


# id, ì¸¡ì •ì¼ì‹œ, ì‘ì—…ìœ í˜• + ì˜ˆì¸¡ëœ í”¼ì²˜ë“¤ ì €ì¥
final_output = test_df[["id", "ì¸¡ì •ì¼ì‹œ", "ì‘ì—…ìœ í˜•"] + MULTI_TARGETS]
final_output.to_csv("test_predicted_december_data.csv", index=False)
print("test_predicted_december_data.csv ì €ì¥ ì™„ë£Œ (ë©€í‹°íƒ€ê¹ƒ ì˜ˆì¸¡ í¬í•¨)")
