
import os
from datetime import datetime

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

try:
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    STATS_AVAILABLE = True
except Exception:
    STATS_AVAILABLE = False

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# ----------------------------------------------------------------------
# 1. Load data
# ----------------------------------------------------------------------
BASE_DIR = "../data"
train_df = pd.read_csv(os.path.join(BASE_DIR, "train.csv"))
test_df = pd.read_csv(os.path.join(BASE_DIR, "test.csv"))
print("âœ”ï¸ [1] ë°ì´í„° ë¡œë”© ì¤‘...")

# ----------------------------------------------------------------------
# 2. Datetime features
# ----------------------------------------------------------------------
for df in [train_df, test_df]:
    df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"])
    df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
    df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
    df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
    df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
    df["ì£¼ë§ì—¬ë¶€"] = (df["ìš”ì¼"] >= 5).astype(int)
    df["sin_ì‹œê°„"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
    df["cos_ì‹œê°„"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)

print("âœ”ï¸ [2] ë‚ ì§œ ë³€ìˆ˜ ì²˜ë¦¬ ì¤‘...")

# ----------------------------------------------------------------------
# 3. Tariff calculation
# ----------------------------------------------------------------------
def get_season(month: int) -> str:
    if month in [6, 7, 8]:
        return "ì—¬ë¦„"
    elif month in [3, 4, 5, 9, 10]:
        return "ë´„ê°€ì„"
    return "ê²¨ìš¸"


def get_time_zone(hour: int, season: str) -> str:
    if season in ["ì—¬ë¦„", "ë´„ê°€ì„"]:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 11) or (12 <= hour < 13) or (18 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 9) or (12 <= hour < 16) or (19 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"


RATE_TABLE = {
    "before": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4},
    },
    "after": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3},
    },
}

CUTOFF = datetime(2024, 10, 24)
for df in [train_df, test_df]:
    df["ê³„ì ˆ"] = df["ì›”"].apply(get_season)
    df["ì ìš©ì‹œì "] = df["ì¸¡ì •ì¼ì‹œ"].apply(lambda x: "before" if x < CUTOFF else "after")
    df["ì‹œê°„ëŒ€"] = df.apply(lambda r: get_time_zone(r["ì‹œê°„"], r["ê³„ì ˆ"]), axis=1)
    df["ìš”ê¸ˆë‹¨ê°€"] = df.apply(lambda r: RATE_TABLE[r["ì ìš©ì‹œì "]][r["ê³„ì ˆ"]][r["ì‹œê°„ëŒ€"]], axis=1)

print("âœ”ï¸ [3] ê³„ì ˆ/ì‹œê°„ëŒ€/ìš”ê¸ˆë‹¨ê°€ ì²˜ë¦¬ ì¤‘...")

# ----------------------------------------------------------------------
# 4. Encoding and target encoding
# ----------------------------------------------------------------------

print("âœ”ï¸ [4] ë¼ë²¨ ì¸ì½”ë”© ë° íƒ€ê²Ÿ ì¸ì½”ë”© ì¤‘...")

le = LabelEncoder()
train_df["ì‘ì—…ìœ í˜•_encoded"] = le.fit_transform(train_df["ì‘ì—…ìœ í˜•"])
test_df["ì‘ì—…ìœ í˜•_encoded"] = le.transform(test_df["ì‘ì—…ìœ í˜•"])


def target_encoding(df_train: pd.DataFrame, df_test: pd.DataFrame, col: str, target: str, smoothing: int = 10) -> None:
    global_mean = df_train[target].mean()
    agg = df_train.groupby(col)[target].agg(["mean", "count"])
    smoothing_weight = 1 / (1 + np.exp(-(agg["count"] - smoothing)))
    enc = global_mean * (1 - smoothing_weight) + agg["mean"] * smoothing_weight
    mapping = enc.to_dict()
    df_train[f"{col}_te"] = df_train[col].map(mapping)
    df_test[f"{col}_te"] = df_test[col].map(mapping)


for c in ["ì‘ì—…ìœ í˜•", "ì‹œê°„", "ìš”ì¼", "ì‹œê°„ëŒ€"]:
    target_encoding(train_df, test_df, c, "ì „ê¸°ìš”ê¸ˆ(ì›)")


# ----------------------------------------------------------------------
# 5. Outlier removal with IQR
# ----------------------------------------------------------------------
print("âœ”ï¸ [5] ì´ìƒì¹˜ ì œê±° ì¤‘...")

q1 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.25)
q3 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
train_df = train_df[(train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] >= lower) & (train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] <= upper)].copy()

# ----------------------------------------------------------------------
# 6. Feature selection
# ----------------------------------------------------------------------
print("âœ”ï¸ [6] í”¼ì²˜ ì„ íƒ ë° ì •ì˜ ì™„ë£Œ")

FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded",
    "ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "ì£¼ë§ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„",
    "ìš”ê¸ˆë‹¨ê°€",
    "ì‘ì—…ìœ í˜•_te", "ì‹œê°„_te", "ìš”ì¼_te", "ì‹œê°„ëŒ€_te",
]
TARGET = "ì „ê¸°ìš”ê¸ˆ(ì›)"

X = train_df[FEATURES]
y = train_df[TARGET]
X_test = test_df[FEATURES]

# ----------------------------------------------------------------------
# 7. Scaling
# ----------------------------------------------------------------------
print("âœ”ï¸ [7] ìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘ (RobustScaler)...")

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

# ----------------------------------------------------------------------
# 8. Tree based models
# ----------------------------------------------------------------------
print("âœ”ï¸ [8] íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸ í•™ìŠµ ì‹œì‘")

X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

models = {
    "xgb": XGBRegressor(n_estimators=400, max_depth=5, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1),
    "lgb": LGBMRegressor(n_estimators=400, max_depth=5, learning_rate=0.03, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, verbose=-1),
    "rf": RandomForestRegressor(n_estimators=300, max_depth=10, random_state=42, n_jobs=-1),
}

preds_val = {}
preds_test = {}
metrics = {}

for name, model in models.items():
    print(f"  ğŸ” {name.upper()} ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model.fit(X_train, y_train)
    print(f"  âœ… {name.upper()} í•™ìŠµ ì™„ë£Œ, R2: {r2_score(y_val, model.predict(X_val)):.4f}")
    val_pred = model.predict(X_val)
    preds_test[name] = model.predict(X_test_scaled)
    preds_val[name] = val_pred
    metrics[name] = r2_score(y_val, val_pred)

# ----------------------------------------------------------------------
# 9. Optional SARIMAX
# ----------------------------------------------------------------------
print("âœ”ï¸ [9] SARIMAX í•™ìŠµ ì‹œë„ ì¤‘...")
if STATS_AVAILABLE:
    print("  ğŸ” SARIMAX ëª¨ë¸ í•™ìŠµ ì‹œì‘")
    sarimax = SARIMAX(y, exog=X_scaled, order=(1,1,1), seasonal_order=(1,1,1,24))
    sarimax_fit = sarimax.fit(disp=False)
    val_pred = sarimax_fit.predict(start=len(y_train), end=len(y_train)+len(y_val)-1, exog=X_val)
    preds_val["sarimax"] = val_pred
    preds_test["sarimax"] = sarimax_fit.predict(start=len(X_scaled), end=len(X_scaled)+len(X_test_scaled)-1, exog=X_test_scaled)
    metrics["sarimax"] = r2_score(y_val, val_pred)
    print(f"  âœ… SARIMAX R2: {metrics['sarimax']:.4f}")
else:
    print("statsmodels not available - skipping SARIMAX")

# ----------------------------------------------------------------------
# 10. LSTM model
# ----------------------------------------------------------------------
print("âœ”ï¸ [10] LSTM ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

TIME_STEPS = 96 * 7

seq_scaler = MinMaxScaler()
seq_data = train_df[FEATURES + [TARGET]].copy()
seq_scaled = seq_scaler.fit_transform(seq_data)
seq_scaled = pd.DataFrame(seq_scaled, columns=FEATURES + [TARGET])

def create_sequences(arr: pd.DataFrame, timesteps: int) -> tuple:
    xs, ys = [], []
    for i in range(len(arr) - timesteps):
        xs.append(arr.iloc[i:i+timesteps][FEATURES].values)
        ys.append(arr.iloc[i+timesteps][TARGET])
    return np.array(xs), np.array(ys)

X_seq, y_seq = create_sequences(seq_scaled, TIME_STEPS)
seq_train, seq_val = int(len(X_seq) * 0.8), int(len(X_seq) * 0.8)
X_seq_train, X_seq_val = X_seq[:seq_train], X_seq[seq_train:]
y_seq_train, y_seq_val = y_seq[:seq_train], y_seq[seq_train:]

print("  ğŸ” LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘ (ìµœëŒ€ 20 epoch)...")

lstm_model = Sequential([
    LSTM(64, input_shape=(TIME_STEPS, len(FEATURES))),
    Dense(32, activation="relu"),
    Dense(1),
])
lstm_model.compile(optimizer="adam", loss="mse")

es = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
lstm_model.fit(X_seq_train, y_seq_train, validation_data=(X_seq_val, y_seq_val), epochs=20, batch_size=32, callbacks=[es], verbose=0)

val_pred = lstm_model.predict(X_seq_val).flatten()
metrics["lstm"] = r2_score(y_seq_val, val_pred)

print(f"  âœ… LSTM R2: {metrics['lstm']:.4f}")

def predict_lstm(model, last_known: pd.DataFrame, future: pd.DataFrame) -> np.ndarray:
    combined = pd.concat([last_known, future], ignore_index=True)
    combined_scaled = seq_scaler.transform(combined)
    combined_scaled = pd.DataFrame(combined_scaled, columns=FEATURES + [TARGET])
    seqs = [combined_scaled.iloc[i:i+TIME_STEPS][FEATURES].values for i in range(len(combined_scaled) - TIME_STEPS)]
    seqs = np.array(seqs)
    preds = model.predict(seqs).flatten()
    return preds[-len(future):]


last_part = train_df[FEATURES + [TARGET]].iloc[-TIME_STEPS:]
lstm_test_pred = predict_lstm(lstm_model, last_part, test_df[FEATURES])

preds_test["lstm"] = lstm_test_pred
preds_val["lstm"] = val_pred

# ----------------------------------------------------------------------
# 11. Weighted ensemble
# ----------------------------------------------------------------------
print("âœ”ï¸ [11] ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ê³„ì‚° ì¤‘...")

weights = {name: max(score, 0) for name, score in metrics.items()}
total = sum(weights.values())
if total == 0:
    weights = {name: 1/len(weights) for name in weights}
else:
    weights = {name: w/total for name, w in weights.items()}

val_ens = np.zeros_like(list(preds_val.values())[0])
for name, pred in preds_val.items():
    val_ens += weights[name] * pred

ens_r2 = r2_score(y_seq_val if "lstm" in preds_val else y_val, val_ens)
print("Ensemble R2:", round(ens_r2, 4))

# Test prediction
test_pred = np.zeros(len(test_df))
for name, pred in preds_test.items():
    test_pred += weights[name] * pred

submission = pd.DataFrame({"id": test_df["id"], "ì „ê¸°ìš”ê¸ˆ(ì›)": test_pred})
submission.to_csv("submission_optimal.csv", index=False)
print("Saved submission_optimal.csv")
