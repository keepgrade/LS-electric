"""Optimized model for LS electric electricity bill prediction.

This script includes advanced optimizations:
- Lag features and rolling statistics for time series patterns
- Log transformation for target distribution
- Improved LSTM architecture with stacking
- Hyperparameter optimization with Optuna
- Enhanced stacking ensemble

Expected MAE improvement: 30-60 points reduction
"""

import os
import pickle
import warnings
from datetime import datetime

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import Ridge
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

# Optuna for hyperparameter optimization
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("Optuna not available - using default hyperparameters")

try:
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    STATS_AVAILABLE = False
except Exception:
    STATS_AVAILABLE = False

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

warnings.filterwarnings('ignore')

# ----------------------------------------------------------------------
# 1. Load data
# ----------------------------------------------------------------------
BASE_DIR = "../data"
train_df = pd.read_csv(os.path.join(BASE_DIR, "train.csv"))
test_df = pd.read_csv(os.path.join(BASE_DIR, "test.csv"))

print(f"Train shape: {train_df.shape}, Test shape: {test_df.shape}")

# ----------------------------------------------------------------------
# 2. Enhanced datetime features
# ----------------------------------------------------------------------
def create_datetime_features(df):
    """Enhanced datetime feature engineering"""
    df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"])
    
    # Basic time features
    df["ë…„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.year
    df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
    df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
    df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
    df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
    df["ì£¼ë§ì—¬ë¶€"] = (df["ìš”ì¼"] >= 5).astype(int)
    
    # Cyclical features
    df["sin_ì‹œê°„"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
    df["cos_ì‹œê°„"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)
    df["sin_ìš”ì¼"] = np.sin(2 * np.pi * df["ìš”ì¼"] / 7)
    df["cos_ìš”ì¼"] = np.cos(2 * np.pi * df["ìš”ì¼"] / 7)
    df["sin_ì›”"] = np.sin(2 * np.pi * df["ì›”"] / 12)
    df["cos_ì›”"] = np.cos(2 * np.pi * df["ì›”"] / 12)
    
    # Advanced time features
    df["ì›”ì´ˆì—¬ë¶€"] = (df["ì¼"] <= 5).astype(int)
    df["ì›”ë§ì—¬ë¶€"] = (df["ì¼"] >= 25).astype(int)
    df["peak_time"] = ((df["ì‹œê°„"] >= 8) & (df["ì‹œê°„"] <= 22)).astype(int)
    df["night_time"] = ((df["ì‹œê°„"] >= 22) | (df["ì‹œê°„"] <= 6)).astype(int)
    
    return df

for df in [train_df, test_df]:
    df = create_datetime_features(df)

# ----------------------------------------------------------------------
# 3. Enhanced tariff calculation
# ----------------------------------------------------------------------
def get_season(month: int) -> str:
    if month in [6, 7, 8]:
        return "ì—¬ë¦„"
    elif month in [3, 4, 5, 9, 10]:
        return "ë´„ê°€ì„"
    return "ê²¨ìš¸"

def get_time_zone(hour: int, season: str) -> str:
    if season in ["ì—¬ë¦„", "ë´„ê°€ì„"]:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 11) or (12 <= hour < 13) or (18 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 9) or (12 <= hour < 16) or (19 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"

RATE_TABLE = {
    "before": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4},
    },
    "after": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3},
    },
}

CUTOFF = datetime(2024, 10, 24)
for df in [train_df, test_df]:
    df["ê³„ì ˆ"] = df["ì›”"].apply(get_season)
    df["ì ìš©ì‹œì "] = df["ì¸¡ì •ì¼ì‹œ"].apply(lambda x: "before" if x < CUTOFF else "after")
    df["ì‹œê°„ëŒ€"] = df.apply(lambda r: get_time_zone(r["ì‹œê°„"], r["ê³„ì ˆ"]), axis=1)
    df["ìš”ê¸ˆë‹¨ê°€"] = df.apply(lambda r: RATE_TABLE[r["ì ìš©ì‹œì "]][r["ê³„ì ˆ"]][r["ì‹œê°„ëŒ€"]], axis=1)

# ----------------------------------------------------------------------
# 4. Encoding and enhanced target encoding
# ----------------------------------------------------------------------
le = LabelEncoder()
train_df["ì‘ì—…ìœ í˜•_encoded"] = le.fit_transform(train_df["ì‘ì—…ìœ í˜•"])
test_df["ì‘ì—…ìœ í˜•_encoded"] = le.transform(test_df["ì‘ì—…ìœ í˜•"])

def target_encoding_with_kfold(df_train: pd.DataFrame, df_test: pd.DataFrame, 
                              col: str, target: str, smoothing: int = 10, n_folds: int = 5) -> None:
    """Enhanced target encoding with K-fold to prevent overfitting"""
    from sklearn.model_selection import KFold
    
    global_mean = df_train[target].mean()
    df_train[f"{col}_te"] = global_mean
    
    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
    for train_idx, val_idx in kf.split(df_train):
        train_fold = df_train.iloc[train_idx]
        val_fold = df_train.iloc[val_idx]
        
        agg = train_fold.groupby(col)[target].agg(["mean", "count"])
        smoothing_weight = 1 / (1 + np.exp(-(agg["count"] - smoothing)))
        enc = global_mean * (1 - smoothing_weight) + agg["mean"] * smoothing_weight
        mapping = enc.to_dict()
        
        df_train.loc[val_idx, f"{col}_te"] = val_fold[col].map(mapping).fillna(global_mean)
    
    # For test set
    agg = df_train.groupby(col)[target].agg(["mean", "count"])
    smoothing_weight = 1 / (1 + np.exp(-(agg["count"] - smoothing)))
    enc = global_mean * (1 - smoothing_weight) + agg["mean"] * smoothing_weight
    mapping = enc.to_dict()
    df_test[f"{col}_te"] = df_test[col].map(mapping).fillna(global_mean)

for c in ["ì‘ì—…ìœ í˜•", "ì‹œê°„", "ìš”ì¼", "ì‹œê°„ëŒ€", "ê³„ì ˆ"]:
    target_encoding_with_kfold(train_df, test_df, c, "ì „ê¸°ìš”ê¸ˆ(ì›)")

# ----------------------------------------------------------------------
# 5. Enhanced time series features (Lag + Rolling)
# ----------------------------------------------------------------------
def create_lag_features(df, target_col, lags=[1, 2, 3, 6, 12, 24]):
    """Create lag features for time series"""
    df_sorted = df.sort_values("ì¸¡ì •ì¼ì‹œ").copy()
    
    for lag in lags:
        df_sorted[f"{target_col}_lag{lag}"] = df_sorted[target_col].shift(lag)
    
    return df_sorted

def create_rolling_features(df, target_col, windows=[3, 6, 12, 24]):
    """Create rolling statistics features"""
    df_sorted = df.sort_values("ì¸¡ì •ì¼ì‹œ").copy()
    
    for window in windows:
        df_sorted[f"{target_col}_roll_mean{window}"] = df_sorted[target_col].rolling(window).mean()
        df_sorted[f"{target_col}_roll_std{window}"] = df_sorted[target_col].rolling(window).std()
        df_sorted[f"{target_col}_roll_max{window}"] = df_sorted[target_col].rolling(window).max()
        df_sorted[f"{target_col}_roll_min{window}"] = df_sorted[target_col].rolling(window).min()
    
    return df_sorted

# Sort both dataframes by datetime
train_df = train_df.sort_values("ì¸¡ì •ì¼ì‹œ").reset_index(drop=True)
test_df = test_df.sort_values("ì¸¡ì •ì¼ì‹œ").reset_index(drop=True)

# Create lag and rolling features for training data
train_df = create_lag_features(train_df, "ì „ê¸°ìš”ê¸ˆ(ì›)")
train_df = create_rolling_features(train_df, "ì „ê¸°ìš”ê¸ˆ(ì›)")

# For test data, we need to be careful about data leakage
# Use the last known values from training data
last_train_values = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].tail(24).values
for i, lag in enumerate([1, 2, 3, 6, 12, 24]):
    if i < len(last_train_values):
        test_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_lag{lag}"] = last_train_values[-(i+1)]
    else:
        test_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_lag{lag}"] = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].mean()

# For rolling features in test, use training data statistics
for window in [3, 6, 12, 24]:
    test_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_mean{window}"] = train_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_mean{window}"].iloc[-1]
    test_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_std{window}"] = train_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_std{window}"].iloc[-1]
    test_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_max{window}"] = train_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_max{window}"].iloc[-1]
    test_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_min{window}"] = train_df[f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_min{window}"].iloc[-1]

# ----------------------------------------------------------------------
# 6. Outlier removal with enhanced IQR
# ----------------------------------------------------------------------
def remove_outliers_iqr(df, target_col, factor=1.5):
    q1 = df[target_col].quantile(0.25)
    q3 = df[target_col].quantile(0.75)
    iqr = q3 - q1
    lower = q1 - factor * iqr
    upper = q3 + factor * iqr
    
    before_count = len(df)
    df_clean = df[(df[target_col] >= lower) & (df[target_col] <= upper)].copy()
    after_count = len(df_clean)
    
    print(f"Outlier removal: {before_count} â†’ {after_count} ({before_count-after_count} removed)")
    return df_clean

train_df = remove_outliers_iqr(train_df, "ì „ê¸°ìš”ê¸ˆ(ì›)", factor=2.0)

# ----------------------------------------------------------------------
# 7. Enhanced feature selection
# ----------------------------------------------------------------------
LAG_FEATURES = [f"ì „ê¸°ìš”ê¸ˆ(ì›)_lag{lag}" for lag in [1, 2, 3, 6, 12, 24]]
ROLLING_FEATURES = []
for window in [3, 6, 12, 24]:
    ROLLING_FEATURES.extend([
        f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_mean{window}",
        f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_std{window}",
        f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_max{window}",
        f"ì „ê¸°ìš”ê¸ˆ(ì›)_roll_min{window}"
    ])

FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded",
    "ë…„", "ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "ì£¼ë§ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„", "sin_ìš”ì¼", "cos_ìš”ì¼", "sin_ì›”", "cos_ì›”",
    "ì›”ì´ˆì—¬ë¶€", "ì›”ë§ì—¬ë¶€", "peak_time", "night_time",
    "ìš”ê¸ˆë‹¨ê°€",
    "ì‘ì—…ìœ í˜•_te", "ì‹œê°„_te", "ìš”ì¼_te", "ì‹œê°„ëŒ€_te", "ê³„ì ˆ_te",
] + LAG_FEATURES + ROLLING_FEATURES

TARGET = "ì „ê¸°ìš”ê¸ˆ(ì›)"

# Remove missing values from training data
train_df = train_df.dropna(subset=FEATURES + [TARGET])

print(f"Final training shape: {train_df.shape}")
print(f"Features count: {len(FEATURES)}")

X = train_df[FEATURES]
y = train_df[TARGET]
X_test = test_df[FEATURES].fillna(train_df[FEATURES].median())

# ----------------------------------------------------------------------
# 8. Log transformation for target
# ----------------------------------------------------------------------
USE_LOG_TRANSFORM = True
if USE_LOG_TRANSFORM:
    print("Applying log transformation to target...")
    y_log = np.log1p(y)
    y_original = y.copy()
    y = y_log

# ----------------------------------------------------------------------
# 9. Enhanced scaling
# ----------------------------------------------------------------------
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

# ----------------------------------------------------------------------
# 10. Hyperparameter optimization with Optuna
# ----------------------------------------------------------------------
def optimize_lightgbm(X_train, X_val, y_train, y_val, n_trials=100):
    """Optimize LightGBM hyperparameters using Optuna"""
    def objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 200, 1000),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0),
            'random_state': 42,
            'n_jobs': -1,
            'verbose': -1
        }
        
        model = LGBMRegressor(**params)
        model.fit(X_train, y_train)
        pred = model.predict(X_val)
        mae = mean_absolute_error(y_val, pred)
        return mae
    
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)
    return study.best_params

# ----------------------------------------------------------------------
# 11. Enhanced tree models with optimization
# ----------------------------------------------------------------------
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Get optimized parameters
if OPTUNA_AVAILABLE:
    print("Optimizing LightGBM hyperparameters...")
    lgb_best_params = optimize_lightgbm(X_train, X_val, y_train, y_val, n_trials=50)
    print(f"Best LightGBM params: {lgb_best_params}")
else:
    lgb_best_params = {
        'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.05,
        'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 1.0,
        'reg_lambda': 1.0, 'random_state': 42, 'n_jobs': -1, 'verbose': -1
    }

models = {
    "xgb": XGBRegressor(
        n_estimators=600, max_depth=7, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, reg_alpha=1.0,
        reg_lambda=1.0, random_state=42, n_jobs=-1
    ),
    "lgb": LGBMRegressor(**lgb_best_params),
    "rf": RandomForestRegressor(
        n_estimators=400, max_depth=12, min_samples_split=5,
        min_samples_leaf=2, random_state=42, n_jobs=-1
    ),
}

preds_val = {}
preds_test = {}
metrics = {}

print("Training enhanced tree models...")
for idx, (name, model) in enumerate(models.items(), 1):
    print(f"[{idx}/{len(models)}] Training {name}...")
    model.fit(X_train, y_train)
    val_pred = model.predict(X_val)
    test_pred = model.predict(X_test_scaled)
    
    preds_val[name] = val_pred
    preds_test[name] = test_pred
    
    if USE_LOG_TRANSFORM:
        val_pred_original = np.expm1(val_pred)
        y_val_original = np.expm1(y_val)
        mae = mean_absolute_error(y_val_original, val_pred_original)
        r2 = r2_score(y_val_original, val_pred_original)
    else:
        mae = mean_absolute_error(y_val, val_pred)
        r2 = r2_score(y_val, val_pred)
    
    metrics[name] = r2
    print(f"[{idx}/{len(models)}] {name} - MAE: {mae:.2f}, R2: {r2:.4f}")

# ----------------------------------------------------------------------
# 12. Optional SARIMAX
# ----------------------------------------------------------------------
if STATS_AVAILABLE:
    print("Training SARIMAX model...")
    try:
        sarimax = SARIMAX(y, exog=X_scaled, order=(1,1,1), seasonal_order=(1,1,1,24))
        sarimax_fit = sarimax.fit(disp=False)
        val_pred = sarimax_fit.predict(start=len(y_train), end=len(y_train)+len(y_val)-1, exog=X_val)
        test_pred = sarimax_fit.predict(start=len(X_scaled), end=len(X_scaled)+len(X_test_scaled)-1, exog=X_test_scaled)
        
        preds_val["sarimax"] = val_pred
        preds_test["sarimax"] = test_pred
        
        if USE_LOG_TRANSFORM:
            val_pred_original = np.expm1(val_pred)
            y_val_original = np.expm1(y_val)
            r2 = r2_score(y_val_original, val_pred_original)
        else:
            r2 = r2_score(y_val, val_pred)
        metrics["sarimax"] = r2
        print(f"SARIMAX R2: {r2:.4f}")
    except Exception as e:
        print(f"SARIMAX failed: {e}")
else:
    print("statsmodels not available - skipping SARIMAX")

# ----------------------------------------------------------------------
# 13. Enhanced LSTM model
# ----------------------------------------------------------------------
TIME_STEPS = 96 * 7  # 1 week

# Prepare sequence data
seq_scaler = MinMaxScaler()
seq_data = train_df[FEATURES + [TARGET]].copy()
seq_scaled = seq_scaler.fit_transform(seq_data)
seq_scaled = pd.DataFrame(seq_scaled, columns=FEATURES + [TARGET])

def create_sequences(arr: pd.DataFrame, timesteps: int) -> tuple:
    xs, ys = [], []
    for i in range(len(arr) - timesteps):
        xs.append(arr.iloc[i:i+timesteps][FEATURES].values)
        ys.append(arr.iloc[i+timesteps][TARGET])
    return np.array(xs), np.array(ys)

X_seq, y_seq = create_sequences(seq_scaled, TIME_STEPS)
seq_train_idx = int(len(X_seq) * 0.8)

X_seq_train, X_seq_val = X_seq[:seq_train_idx], X_seq[seq_train_idx:]
y_seq_train, y_seq_val = y_seq[:seq_train_idx], y_seq[seq_train_idx:]

# Enhanced LSTM architecture
lstm_model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(TIME_STEPS, len(FEATURES))),
    Dropout(0.2),
    LSTM(64, return_sequences=False),
    Dropout(0.2),
    Dense(32, activation="relu"),
    Dropout(0.1),
    Dense(1),
])

lstm_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="mse",
    metrics=["mae"]
)

# Enhanced callbacks
es = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=5, min_lr=1e-6)

print("Training enhanced LSTM model...")
history = lstm_model.fit(
    X_seq_train, y_seq_train,
    validation_data=(X_seq_val, y_seq_val),
    epochs=14,
    batch_size=64,
    callbacks=[es, reduce_lr],
    verbose=1,
)

# LSTM predictions
val_pred = lstm_model.predict(X_seq_val).flatten()
preds_val["lstm"] = val_pred

if USE_LOG_TRANSFORM:
    val_pred_original = np.expm1(val_pred)
    y_seq_val_original = np.expm1(y_seq_val)
    r2 = r2_score(y_seq_val_original, val_pred_original)
else:
    r2 = r2_score(y_seq_val, val_pred)
metrics["lstm"] = r2

# LSTM test prediction
def predict_lstm_enhanced(model, last_known: pd.DataFrame, future: pd.DataFrame) -> np.ndarray:
    combined = pd.concat([last_known, future], ignore_index=True)
    combined_scaled = seq_scaler.transform(combined)
    combined_scaled = pd.DataFrame(combined_scaled, columns=FEATURES + [TARGET])
    
    predictions = []
    for i in range(len(future)):
        if i + TIME_STEPS < len(combined_scaled):
            seq = combined_scaled.iloc[i:i+TIME_STEPS][FEATURES].values.reshape(1, TIME_STEPS, -1)
            pred = model.predict(seq, verbose=0)[0, 0]
            predictions.append(pred)
    
    return np.array(predictions)

last_part = train_df[FEATURES + [TARGET]].iloc[-TIME_STEPS:]
lstm_test_pred = predict_lstm_enhanced(lstm_model, last_part, test_df[FEATURES])
preds_test["lstm"] = lstm_test_pred

print(f"LSTM R2: {metrics['lstm']:.4f}")

# ----------------------------------------------------------------------
# 14. Advanced Stacking Ensemble
# ----------------------------------------------------------------------
print("Creating stacking ensemble...")

# 1. ê¸¸ì´ ì •ë ¬
min_len = min(len(pred) for pred in preds_val.values())
level1_train = np.column_stack([pred[:min_len] for pred in preds_val.values()])
level1_test = np.column_stack([preds_test[name] for name in preds_test.keys()])

# 2. íƒ€ê¹ƒ ì •ë ¬
y_meta = y_seq_val if "lstm" in preds_val else y_val
y_meta_aligned = y_meta[:min_len]

# 3. ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
meta_model = Ridge(alpha=1.0, random_state=42)
meta_model.fit(level1_train, y_meta_aligned)

# 4. ì˜ˆì¸¡ ë° í‰ê°€
ensemble_val = meta_model.predict(level1_train)
ensemble_test = meta_model.predict(level1_test)

if USE_LOG_TRANSFORM:
    ensemble_val_original = np.expm1(ensemble_val)
    y_meta_original = np.expm1(y_meta_aligned)
    ensemble_test_original = np.expm1(ensemble_test)
else:
    ensemble_val_original = ensemble_val
    y_meta_original = y_meta_aligned
    ensemble_test_original = ensemble_test

ensemble_mae = mean_absolute_error(y_meta_original, ensemble_val_original)
ensemble_r2 = r2_score(y_meta_original, ensemble_val_original)

# 5. ì¶œë ¥
print(f"ğŸ¯ Stacking Ensemble - MAE: {ensemble_mae:.2f}, R2: {ensemble_r2:.4f}")
print("\nğŸ“Š Individual Model Performance:")
for name, r2 in metrics.items():
    print(f"  {name}: R2 = {r2:.4f}")
print(f"\nğŸ† Final Ensemble: MAE = {ensemble_mae:.2f}, R2 = {ensemble_r2:.4f}")
# ----------------------------------------------------------------------
# 15. Save predictions
# ----------------------------------------------------------------------
if USE_LOG_TRANSFORM:
    final_predictions = ensemble_test_original
else:
    final_predictions = ensemble_test

submission = pd.DataFrame({
    "id": test_df["id"], 
    "ì „ê¸°ìš”ê¸ˆ(ì›)": final_predictions
})

submission.to_csv("submission_optimized.csv", index=False)
submission.to_csv("submission.csv", index=False)
print("âœ… Saved submission_optimized.csv and submission.csv")

# ----------------------------------------------------------------------
# 16. Save trained models
# ----------------------------------------------------------------------
MODELS_DIR = "pickles_optimized"
os.makedirs(MODELS_DIR, exist_ok=True)

print("ğŸ’¾ Saving optimized models...")
for name, model in models.items():
    with open(os.path.join(MODELS_DIR, f"{name}.pkl"), "wb") as f:
        pickle.dump(model, f)

if STATS_AVAILABLE and "sarimax" in preds_test:
    with open(os.path.join(MODELS_DIR, "sarimax.pkl"), "wb") as f:
        pickle.dump(sarimax_fit, f)

lstm_model.save(os.path.join(MODELS_DIR, "lstm_model.h5"))

with open(os.path.join(MODELS_DIR, "meta_model.pkl"), "wb") as f:
    pickle.dump(meta_model, f)

with open(os.path.join(MODELS_DIR, "scaler.pkl"), "wb") as f:
    pickle.dump(scaler, f)

with open(os.path.join(MODELS_DIR, "seq_scaler.pkl"), "wb") as f:
    pickle.dump(seq_scaler, f)

with open(os.path.join(MODELS_DIR, "label_encoder.pkl"), "wb") as f:
    pickle.dump(le, f)

print(f"âœ… All models saved to {MODELS_DIR}")

# ----------------------------------------------------------------------
# 17. Multi-target prediction (keeping original functionality)
# ----------------------------------------------------------------------
MULTI_TARGETS = [
    "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)",
    "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)",
    "íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)",
    "ì§€ìƒì—­ë¥ (%)",
    "ì§„ìƒì—­ë¥ (%)",
    "ì „ê¸°ìš”ê¸ˆ(ì›)"
]

# test_df ìƒì„± ë° íŒŒìƒ ë³€ìˆ˜ í¬í•¨ ì™„ë£Œëœ ìƒíƒœë¼ê³  ê°€ì •
test_pred_dict = {}

for target in MULTI_TARGETS:
    print(f"ğŸ” [{target}] ì˜ˆì¸¡ ì¤‘...")
    
    # í•™ìŠµìš© ë°ì´í„° êµ¬ì„±
    y_multi = train_df[target]
    X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_multi, test_size=0.2, random_state=42)

    # ëª¨ë¸ í›ˆë ¨ (ì—¬ê¸°ì„  LightGBM ì‚¬ìš© ì˜ˆì‹œ)
    model = LGBMRegressor(n_estimators=300, random_state=42)
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    y_test_pred = model.predict(X_test_scaled)
    test_pred_dict[target] = y_test_pred
    print(f"âœ… [{target}] ì˜ˆì¸¡ ì™„ë£Œ")

for target, pred in test_pred_dict.items():
    test_df[target] = pred


# id, ì¸¡ì •ì¼ì‹œ, ì‘ì—…ìœ í˜• + ì˜ˆì¸¡ëœ í”¼ì²˜ë“¤ ì €ì¥
final_output = test_df[["id", "ì¸¡ì •ì¼ì‹œ", "ì‘ì—…ìœ í˜•"] + MULTI_TARGETS]
final_output.to_csv("test_predicted_december_data.csv", index=False)
print("test_predicted_december_data.csv ì €ì¥ ì™„ë£Œ (ë©€í‹°íƒ€ê¹ƒ ì˜ˆì¸¡ í¬í•¨)")
