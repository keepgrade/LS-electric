"""Optimal model for LS electric electricity bill prediction.

This script combines advanced preprocessing steps with multiple models:
- LSTM on 96x7 time windows
- Tree based models (XGB, LightGBM, RandomForest)
- Optional SARIMAX if statsmodels is installed

The final prediction is an ensemble of the available models.
"""

import os
import pickle
from datetime import datetime

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from scipy.ndimage import uniform_filter1d
try:
    from statsmodels.tsa.statespace.sarimax import SARIMAX
    STATS_AVAILABLE = False
except Exception:
    STATS_AVAILABLE = False

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import make_scorer
# ----------------------------------------------------------------------
# 1. Load data
# ----------------------------------------------------------------------
BASE_DIR = "../data"
train_df = pd.read_csv(os.path.join(BASE_DIR, "train.csv"))
test_df = pd.read_csv(os.path.join(BASE_DIR, "test.csv"))
# train_df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(train_df["ì¸¡ì •ì¼ì‹œ"])
# test_df = train_df[(train_df["ì¸¡ì •ì¼ì‹œ"] >= "2024-11-01") & (train_df["ì¸¡ì •ì¼ì‹œ"] < "2024-12-01")]
# answer = test_df[["id","ì „ê¸°ìš”ê¸ˆ(ì›)"]]
# test_df = test_df[["ì¸¡ì •ì¼ì‹œ","id","ì‘ì—…ìœ í˜•"]]
# train_df = train_df[train_df["ì¸¡ì •ì¼ì‹œ"] < "2024-11-01"]
# ----------------------------------------------------------------------
# 2. Datetime features
# ----------------------------------------------------------------------
for df in [train_df, test_df]:
    df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"])
    df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
    df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
    df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
    df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
    df["ì£¼ë§ì—¬ë¶€"] = df["ìš”ì¼"].isin([0,6]).astype(int)
    df["sin_ì‹œê°„"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
    df["cos_ì‹œê°„"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)

# ----------------------------------------------------------------------
# 3. Tariff calculation
# ----------------------------------------------------------------------
def get_season(month: int) -> str:
    if month in [6, 7, 8]:
        return "ì—¬ë¦„"
    elif month in [3, 4, 5, 9, 10]:
        return "ë´„ê°€ì„"
    return "ê²¨ìš¸"


def get_time_zone(hour: int, season: str) -> str:
    if season in ["ì—¬ë¦„", "ë´„ê°€ì„"]:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 11) or (12 <= hour < 13) or (18 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8:
            return "ê²½ë¶€í•˜"
        if (8 <= hour < 9) or (12 <= hour < 16) or (19 <= hour < 22):
            return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"


RATE_TABLE = {
    "before": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4},
    },
    "after": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3},
    },
}

CUTOFF = datetime(2024, 10, 24)
for df in [train_df, test_df]:
    df["ê³„ì ˆ"] = df["ì›”"].apply(get_season)
    df["ì ìš©ì‹œì "] = df["ì¸¡ì •ì¼ì‹œ"].apply(lambda x: "before" if x < CUTOFF else "after")
    df["ì‹œê°„ëŒ€"] = df.apply(lambda r: get_time_zone(r["ì‹œê°„"], r["ê³„ì ˆ"]), axis=1)
    df["ìš”ê¸ˆë‹¨ê°€"] = df.apply(lambda r: RATE_TABLE[r["ì ìš©ì‹œì "]][r["ê³„ì ˆ"]][r["ì‹œê°„ëŒ€"]], axis=1)

# ----------------------------------------------------------------------
# 4. Encoding and target encoding
# ----------------------------------------------------------------------
le = LabelEncoder()
train_df["ì‘ì—…ìœ í˜•_encoded"] = le.fit_transform(train_df["ì‘ì—…ìœ í˜•"])
test_df["ì‘ì—…ìœ í˜•_encoded"] = le.transform(test_df["ì‘ì—…ìœ í˜•"])


def target_encoding(df_train: pd.DataFrame, df_test: pd.DataFrame, col: str, target: str, smoothing: int = 10) -> None:
    global_mean = df_train[target].mean()
    agg = df_train.groupby(col)[target].agg(["mean", "count"])
    smoothing_weight = 1 / (1 + np.exp(-(agg["count"] - smoothing)))
    enc = global_mean * (1 - smoothing_weight) + agg["mean"] * smoothing_weight
    mapping = enc.to_dict()
    df_train[f"{col}_te"] = df_train[col].map(mapping)
    df_test[f"{col}_te"] = df_test[col].map(mapping)


for c in ["ì‘ì—…ìœ í˜•", "ì‹œê°„", "ìš”ì¼", "ì‹œê°„ëŒ€"]:
    target_encoding(train_df, test_df, c, "ì „ê¸°ìš”ê¸ˆ(ì›)")

# ----------------------------------------------------------------------
# 5. Outlier removal with IQR
# ----------------------------------------------------------------------
q1 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.05)
q3 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.95)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
train_df = train_df[(train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] >= lower) & (train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] <= upper)].copy()

# ----------------------------------------------------------------------
# 6. Feature selection
# ----------------------------------------------------------------------
FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded",
    "ì›”", "ìš”ì¼", "ì£¼ë§ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„",
    "ìš”ê¸ˆë‹¨ê°€",
   "ì‹œê°„_te"
]
TARGET = "ì „ê¸°ìš”ê¸ˆ(ì›)"

X = train_df[FEATURES]
y = train_df[TARGET]
X_test = test_df[FEATURES]

# ----------------------------------------------------------------------
# 7. Scaling
# ----------------------------------------------------------------------
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

# ----------------------------------------------------------------------
# 8. Tree based models
# ----------------------------------------------------------------------
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)

# íƒìƒ‰ ë²”ìœ„ ì •ì˜
param_grids = {
    "xgb": {
        "n_estimators": [200, 400, 600],
        "max_depth": [3, 5, 7],
        "learning_rate": [0.01, 0.03, 0.1],
        "subsample": [0.7, 0.8, 1.0],
        "colsample_bytree": [0.7, 0.8, 1.0],
    },
    "lgb": {
        "n_estimators": [200, 400, 600],
        "max_depth": [3, 5, 7],
        "learning_rate": [0.01, 0.03, 0.1],
        "subsample": [0.7, 0.8, 1.0],
        "colsample_bytree": [0.7, 0.8, 1.0],
    },
    "rf": {
        "n_estimators": [100, 200, 300],
        "max_depth": [5, 10, 15],
        "max_features": ["sqrt", "log2", None],
    }
}

# ëª¨ë¸ ì •ì˜
model_classes = {
    "xgb": XGBRegressor(random_state=42, n_jobs=-1),
    "lgb": LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),
    "rf": RandomForestRegressor(random_state=42, n_jobs=-1)
}

# ìµœì  ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
best_models = {}
preds_val = {}
preds_test = {}
metrics = {}

for name in ["xgb", "lgb", "rf"]:
    print(f"ğŸ” Tuning {name.upper()}...")
    search = RandomizedSearchCV(
        model_classes[name],
        param_distributions=param_grids[name],
        n_iter=10,
        scoring=mae_scorer,
        cv=3,
        random_state=42,
        n_jobs=-1,
        verbose=1
    )
    search.fit(X_train, y_train)

    best_model = search.best_estimator_
    best_models[name] = best_model

    val_pred = best_model.predict(X_val)
    test_pred = best_model.predict(X_test_scaled)

    preds_val[name] = val_pred
    preds_test[name] = test_pred
    metrics[name] = r2_score(y_val, val_pred)

    print(f"âœ… {name.upper()} Best Params: {search.best_params_}")
    print(f"ğŸ“Š {name.upper()} RÂ²: {metrics[name]:.4f}")

# ----------------------------------------------------------------------
# 9. Optional SARIMAX
# ----------------------------------------------------------------------
if STATS_AVAILABLE:
    sarimax = SARIMAX(y, exog=X_scaled, order=(1,1,1), seasonal_order=(1,1,1,24))
    sarimax_fit = sarimax.fit(disp=False)
    val_pred = sarimax_fit.predict(start=len(y_train), end=len(y_train)+len(y_val)-1, exog=X_val)
    preds_val["sarimax"] = val_pred
    preds_test["sarimax"] = sarimax_fit.predict(start=len(X_scaled), end=len(X_scaled)+len(X_test_scaled)-1, exog=X_test_scaled)
    metrics["sarimax"] = r2_score(y_val, val_pred)
else:
    print("statsmodels not available - skipping SARIMAX")

# ----------------------------------------------------------------------
# 10. LSTM model
# ----------------------------------------------------------------------
TIME_STEPS = 96 * 7
FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded",
    "ì›”","ìš”ì¼", "ì£¼ë§ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„"
]
seq_scaler = MinMaxScaler()
seq_data = train_df[FEATURES + [TARGET]].copy()
seq_scaled = seq_scaler.fit_transform(seq_data)
seq_scaled = pd.DataFrame(seq_scaled, columns=FEATURES + [TARGET])

def create_sequences(arr: pd.DataFrame, timesteps: int) -> tuple:
    xs, ys = [], []
    for i in range(len(arr) - timesteps):
        xs.append(arr.iloc[i:i+timesteps][FEATURES].values)
        ys.append(arr.iloc[i+timesteps][TARGET])
    return np.array(xs), np.array(ys)

X_seq, y_seq = create_sequences(seq_scaled, TIME_STEPS)
seq_train, seq_val = int(len(X_seq) * 0.8), int(len(X_seq) * 0.8)
X_seq_train, X_seq_val = X_seq[:seq_train], X_seq[seq_train:]
y_seq_train, y_seq_val = y_seq[:seq_train], y_seq[seq_train:]

lstm_model = Sequential([
    LSTM(64, input_shape=(TIME_STEPS, len(FEATURES))),
    Dense(32, activation="relu"),
    Dense(1),
])
lstm_model.compile(optimizer="adam", loss="mse")

es = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)
print("Training LSTM model...")
lstm_model.fit(
    X_seq_train,
    y_seq_train,
    validation_data=(X_seq_val, y_seq_val),
    epochs=20,
    batch_size=32,
    callbacks=[es],
    verbose=1,
)

val_pred = lstm_model.predict(X_seq_val).flatten()

dummy_val = np.zeros((len(val_pred), len(FEATURES) + 1))
dummy_val[:, -1] = val_pred
val_pred_inversed = seq_scaler.inverse_transform(dummy_val)[:, -1]

dummy_true = np.zeros((len(y_seq_val), len(FEATURES) + 1))
dummy_true[:, -1] = y_seq_val
y_true_inversed = seq_scaler.inverse_transform(dummy_true)[:, -1]

residual_val = y_true_inversed - val_pred_inversed

val_features = seq_scaled.iloc[-len(val_pred):][FEATURES].copy()
val_features["lstm_pred"] = val_pred_inversed
residual_model = XGBRegressor(n_estimators=200, max_depth=4, learning_rate=0.1)
residual_model.fit(val_features, residual_val)

test_features = test_df[FEATURES].copy()


dummy1 = np.zeros((len(val_pred), len(FEATURES) + 1))  # +1ì€ TARGET(ì „ê¸°ìš”ê¸ˆ)
dummy1[:, -1] = val_pred  # ì „ê¸°ìš”ê¸ˆë§Œ ì±„ìš°ê³  ë‚˜ë¨¸ì§€ëŠ” 0

# 2. ì—­ë³€í™˜ ìˆ˜í–‰
inversed1 = seq_scaler.inverse_transform(dummy1)[:, -1]  # ë§ˆì§€ë§‰ ì—´ë§Œ ì¶”ì¶œ (ì „ê¸°ìš”ê¸ˆ)

metrics["lstm"] = r2_score(y_seq_val, val_pred)

def predict_lstm(model, last_known: pd.DataFrame, future: pd.DataFrame) -> np.ndarray:
    combined = pd.concat([last_known, future], ignore_index=True)
    combined_scaled = seq_scaler.transform(combined)
    combined_scaled = pd.DataFrame(combined_scaled, columns=FEATURES + [TARGET])
    seqs = [combined_scaled.iloc[i:i+TIME_STEPS][FEATURES].values for i in range(len(combined_scaled) - TIME_STEPS)]
    seqs = np.array(seqs)
    preds = model.predict(seqs).flatten()
    return preds[-len(future):]

last_part = train_df[FEATURES + [TARGET]].iloc[-TIME_STEPS:]
lstm_test_pred = predict_lstm(lstm_model, last_part, test_df[FEATURES])

dummy = np.zeros((len(lstm_test_pred), len(FEATURES) + 1))  # +1ì€ TARGET(ì „ê¸°ìš”ê¸ˆ)
dummy[:, -1] = lstm_test_pred  # ì „ê¸°ìš”ê¸ˆë§Œ ì±„ìš°ê³  ë‚˜ë¨¸ì§€ëŠ” 0

# 2. ì—­ë³€í™˜ ìˆ˜í–‰
inversed = seq_scaler.inverse_transform(dummy)[:, -1]  # ë§ˆì§€ë§‰ ì—´ë§Œ ì¶”ì¶œ (ì „ê¸°ìš”ê¸ˆ)
# 1. ì›ë³¸ ì˜ˆì¸¡ (ì´ë¯¸ ì¡´ì¬)
inversed_raw = inversed

test_features["lstm_pred"] = inversed
residual_correction = residual_model.predict(test_features)
corrected_lstm = inversed + residual_correction

# 2. Smoothë§Œ ì ìš©
inversed_smooth = uniform_filter1d(inversed_raw, size=5)

# 3. Clipë§Œ ì ìš© (ì‹¤ì œ lower_boundëŠ” ì•ì„œ ì¶”ì²œí•œ 300ìœ¼ë¡œ ì‚¬ìš©)
lower_bound = 600
upper_bound = 13651.88
inversed_clipped = np.clip(inversed_raw, lower_bound, upper_bound)

# 4. Smooth + Clip ì ìš©
inversed_smooth_clipped = np.clip(inversed_smooth, lower_bound, upper_bound)
# true = answer["ì „ê¸°ìš”ê¸ˆ(ì›)"].values
# MAE ê³„ì‚°
# mae_smooth = mean_absolute_error(true, inversed_smooth)
# mae_clipped = mean_absolute_error(true, inversed_clipped)
# mae_smooth_clipped = mean_absolute_error(true, inversed_smooth_clipped)

# mae_lstm = mean_absolute_error(true, inversed)
# print(f"lstm_mae : {mae_lstm}")

# mae_lstm = mean_absolute_error(true, corrected_lstm)
# print(f"lstm_mae : {mae_lstm}")

preds_test["lstm"] = inversed_clipped
preds_val["lstm"] = inversed1

# ----------------------------------------------------------------------
# 11. Weighted ensemble
# ----------------------------------------------------------------------
weights = {name: max(score, 0) for name, score in metrics.items()}
print("ğŸ“Š ê° ëª¨ë¸ì˜ ì„¤ëª…ë ¥ (RÂ² score):")
for name, score in sorted(metrics.items(), key=lambda x: -x[1]):
    print(f"- {name}: RÂ² = {score:.4f}")
total = sum(weights.values())
if total == 0:
    weights = {name: 1/len(weights) for name in weights}
else:
    weights = {name: w/total for name, w in weights.items()}



# Determine correct validation target
is_seq = "lstm" in preds_val
y_true = y_seq_val if is_seq else y_val
target_len = len(y_true)

# Initialize
val_ens = np.zeros(target_len)

# Ensemble
for name, pred in preds_val.items():
    if len(pred) != target_len:
        print(f"âš ï¸ {name} prediction length mismatch: {len(pred)} vs {target_len}")
        continue
    val_ens += weights[name] * pred

# Test prediction
test_pred = np.zeros(len(test_df))
for name, pred in preds_test.items():
    # mae = mean_absolute_error(true, pred)
    print(f"{name}:{mae}")
    test_pred += weights[name] * pred


submission = pd.DataFrame({
    "id": test_df["id"].values,
    "ì „ê¸°ìš”ê¸ˆ(ì›)": test_pred
})

# submission1 = pd.DataFrame({
#     "id": test_df["id"].values,
#     "ì¸¡ì •ì¼ì‹œ" : test_df["ì¸¡ì •ì¼ì‹œ"],
#     "ì „ê¸°ìš”ê¸ˆ(ì›)": test_pred,
#     "ì‹¤ì œìš”ê¸ˆ":answer["ì „ê¸°ìš”ê¸ˆ(ì›)"]
# })
# for name, pred in preds_test.items():
#     submission1[f"{name}"] = pred
# mae_asem = mean_absolute_error(answer["ì „ê¸°ìš”ê¸ˆ(ì›)"], submission["ì „ê¸°ìš”ê¸ˆ(ì›)"])
# print(f"asem_mae : {mae_asem}")
# submission1.to_csv("submission_optimal.csv", index=False)
submission.to_csv("submission3.csv", index=False)
print("Saved submission_optimal.csv and submission.csv")

# ----------------------------------------------------------------------
# 12. Save trained models
# ----------------------------------------------------------------------
MODELS_DIR = "pickles"
os.makedirs(MODELS_DIR, exist_ok=True)
print("Saving trained models...")
for name, model in models.items():
    with open(os.path.join(MODELS_DIR, f"{name}.pkl"), "wb") as f:
        pickle.dump(model, f)
    print(f"Saved {name}.pkl")
if STATS_AVAILABLE:
    with open(os.path.join(MODELS_DIR, "sarimax.pkl"), "wb") as f:
        pickle.dump(sarimax_fit, f)
    print("Saved sarimax.pkl")
with open(os.path.join(MODELS_DIR, "lstm.pkl"), "wb") as f:
    pickle.dump(lstm_model, f)
print("Saved lstm.pkl")
with open(os.path.join(MODELS_DIR, "scaler.pkl"), "wb") as f:
    pickle.dump(scaler, f)
print("Saved scaler.pkl")
with open(os.path.join(MODELS_DIR, "seq_scaler.pkl"), "wb") as f:
    pickle.dump(seq_scaler, f)
print("Saved seq_scaler.pkl")
print(f"Saved trained models to {MODELS_DIR}")

