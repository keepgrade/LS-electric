# ğŸ“¦ 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import os
import pickle
import numpy as np
import pandas as pd

from datetime import datetime
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# ğŸ“‚ 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
BASE_DIR = "./data"
train_df = pd.read_csv(os.path.join(BASE_DIR, "train_with_weather.csv"))
test_df = pd.read_csv(os.path.join(BASE_DIR, "test_with_weather.csv"))

# ğŸ“… 3. ì‹œê°„ ê´€ë ¨ íŒŒìƒë³€ìˆ˜ ìƒì„±
for df in [train_df, test_df]:
    df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"])
    df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
    df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
    df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
    df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
    df["íœ´ì¼ì—¬ë¶€"] = df["ìš”ì¼"].isin([0, 6]).astype(int)
    df["sin_ì‹œê°„"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
    df["cos_ì‹œê°„"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)

# ğŸ’° 4. ì‹œê°„ëŒ€ ê¸°ë°˜ ìš”ê¸ˆë‹¨ê°€ ìƒì„±
def get_season(month):
    if month in [6, 7, 8]: return "ì—¬ë¦„"
    elif month in [3, 4, 5, 9, 10]: return "ë´„ê°€ì„"
    else: return "ê²¨ìš¸"

def get_time_zone(hour, season):
    if season in ["ì—¬ë¦„", "ë´„ê°€ì„"]:
        if 22 <= hour or hour < 8: return "ê²½ë¶€í•˜"
        if (8 <= hour < 11) or (12 <= hour < 13) or (18 <= hour < 22): return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8: return "ê²½ë¶€í•˜"
        if (8 <= hour < 9) or (12 <= hour < 16) or (19 <= hour < 22): return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"

RATE_TABLE = {
    "before": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4},
    },
    "after": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3},
    },
}
CUTOFF = datetime(2024, 10, 24)
for df in [train_df, test_df]:
    df["ê³„ì ˆ"] = df["ì›”"].apply(get_season)
    df["ì ìš©ì‹œì "] = df["ì¸¡ì •ì¼ì‹œ"].apply(lambda x: "before" if x < CUTOFF else "after")
    df["ì‹œê°„ëŒ€"] = df.apply(lambda r: get_time_zone(r["ì‹œê°„"], r["ê³„ì ˆ"]), axis=1)
    df["ìš”ê¸ˆë‹¨ê°€"] = df.apply(lambda r: RATE_TABLE[r["ì ìš©ì‹œì "]][r["ê³„ì ˆ"]][r["ì‹œê°„ëŒ€"]], axis=1)

# ğŸ”¡ 5. ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© ë° íƒ€ê²Ÿ ì¸ì½”ë”©
def target_encoding(df_train, df_test, col, target, smoothing=10):
    global_mean = df_train[target].mean()
    agg = df_train.groupby(col)[target].agg(["mean", "count"])
    weight = 1 / (1 + np.exp(-(agg["count"] - smoothing)))
    enc = global_mean * (1 - weight) + agg["mean"] * weight
    mapping = enc.to_dict()
    df_train[f"{col}_te"] = df_train[col].map(mapping)
    df_test[f"{col}_te"] = df_test[col].map(mapping)

le = LabelEncoder()
train_df["ì‘ì—…ìœ í˜•_encoded"] = le.fit_transform(train_df["ì‘ì—…ìœ í˜•"])
test_df["ì‘ì—…ìœ í˜•_encoded"] = le.transform(test_df["ì‘ì—…ìœ í˜•"])

for col in ["ì‘ì—…ìœ í˜•", "ì‹œê°„", "ìš”ì¼", "ì‹œê°„ëŒ€"]:
    target_encoding(train_df, test_df, col, "ì „ê¸°ìš”ê¸ˆ(ì›)")

# ğŸ“Š 6. ì´ìƒì¹˜ ì œê±°
q1 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.25)
q3 = train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"].quantile(0.75)
iqr = q3 - q1
lower = q1 - 1.5 * iqr
upper = q3 + 1.5 * iqr
train_df = train_df[(train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] >= lower) & (train_df["ì „ê¸°ìš”ê¸ˆ(ì›)"] <= upper)]

# ğŸ§¾ 7. Feature/Target ì •ì˜ ë° ì •ê·œí™”
FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded", "ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "íœ´ì¼ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„", "ìš”ê¸ˆë‹¨ê°€",
    "ì‘ì—…ìœ í˜•_te", "ì‹œê°„_te", "ìš”ì¼_te", "ì‹œê°„ëŒ€_te"
]
TARGETS = ["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)", "íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)", "ì „ê¸°ìš”ê¸ˆ(ì›)"]

X = train_df[FEATURES]
y = train_df[TARGETS]
X_test = test_df[FEATURES]

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)
X_test_scaled = scaler.transform(X_test)

# ğŸŒ² 8. íŠ¸ë¦¬ ëª¨ë¸ í•™ìŠµ (3ê°œ íƒ€ê²Ÿë³„ ê°œë³„ ì˜ˆì¸¡)
models = {}
metrics = {}
preds_test = {}
X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

for target in TARGETS:
    for name, model in {
        "xgb": XGBRegressor(n_estimators=200, max_depth=4),
        "lgb": LGBMRegressor(n_estimators=200, max_depth=4),
        "rf": RandomForestRegressor(n_estimators=200, max_depth=6)
    }.items():
        model.fit(X_train, y_train[target])
        pred = model.predict(X_val)
        score = r2_score(y_val[target], pred)
        models[f"{name}_{target}"] = model
        metrics[f"{name}_{target}"] = round(score, 4)
        preds_test[f"{name}_{target}"] = model.predict(X_test_scaled)

# ğŸ“ˆ 9. LSTMì„ ìœ„í•œ ì‹œê³„ì—´ ë°ì´í„° êµ¬ì„± ë° ì •ê·œí™”
TIME_STEPS = 96 * 7
seq_scaler = MinMaxScaler()
seq_data = pd.concat([train_df[FEATURES], train_df[TARGETS]], axis=1)
seq_scaled = seq_scaler.fit_transform(seq_data)
scaled_df = pd.DataFrame(seq_scaled, columns=FEATURES + TARGETS)

# ğŸ”„ 10. ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_sequences(data, timesteps):
    xs, ys = [], []
    for i in range(len(data) - timesteps):
        xs.append(data.iloc[i:i+timesteps][FEATURES].values)
        ys.append(data.iloc[i+timesteps][TARGETS].values)
    return np.array(xs), np.array(ys)

X_seq, y_seq = create_sequences(scaled_df, TIME_STEPS)
split = int(0.8 * len(X_seq))
X_seq_train, X_seq_val = X_seq[:split], X_seq[split:]
y_seq_train, y_seq_val = y_seq[:split], y_seq[split:]

# ğŸ§  11. Multi-output LSTM ëª¨ë¸ ì •ì˜ ë° í•™ìŠµ
model = Sequential([
    LSTM(128, input_shape=(TIME_STEPS, len(FEATURES))),
    Dense(64, activation='relu'),
    Dense(len(TARGETS))
])
model.compile(optimizer='adam', loss='mse')
es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
model.fit(X_seq_train, y_seq_train, validation_data=(X_seq_val, y_seq_val),
          epochs=20, batch_size=32, callbacks=[es], verbose=1)

# ğŸ’¾ 12. ëª¨ë¸ ë° ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
os.makedirs("./pickles", exist_ok=True)
with open("./pickles/scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)
with open("./pickles/seq_scaler.pkl", "wb") as f:
    pickle.dump(seq_scaler, f)
model.save("./pickles/lstm_multi_output.h5")

print("âœ… í•™ìŠµ ì™„ë£Œ ë° ëª¨ë¸ ì €ì¥ ì™„ë£Œ")













# âœ… ìŠ¬ë¼ì´ë”© ë°©ì‹ìœ¼ë¡œ 12ì›” ìš”ê¸ˆ ì˜ˆì¸¡ ìˆ˜í–‰ (full_scaled ì €ì¥ ì—†ì´)
import os
import pickle
import numpy as np
import pandas as pd
from datetime import datetime
from tensorflow.keras.models import load_model
from sklearn.preprocessing import LabelEncoder

# ğŸ“‚ 1. ë°ì´í„° ë¡œë“œ
BASE_DIR = "./data"
train_df = pd.read_csv(os.path.join(BASE_DIR, "train_with_weather.csv"))
test_df = pd.read_csv(os.path.join(BASE_DIR, "test_with_weather.csv"))

# ğŸ“… 2. ì‹œê°„ íŒŒìƒ ë³€ìˆ˜ ìƒì„±
for df in [train_df, test_df]:
    df["ì¸¡ì •ì¼ì‹œ"] = pd.to_datetime(df["ì¸¡ì •ì¼ì‹œ"])
    df["ì›”"] = df["ì¸¡ì •ì¼ì‹œ"].dt.month
    df["ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.day
    df["ì‹œê°„"] = df["ì¸¡ì •ì¼ì‹œ"].dt.hour
    df["ìš”ì¼"] = df["ì¸¡ì •ì¼ì‹œ"].dt.weekday
    df["ì£¼ë§ì—¬ë¶€"] = (df["ìš”ì¼"] >= 5).astype(int)
    df["sin_ì‹œê°„"] = np.sin(2 * np.pi * df["ì‹œê°„"] / 24)
    df["cos_ì‹œê°„"] = np.cos(2 * np.pi * df["ì‹œê°„"] / 24)

# ğŸ’° 3. ìš”ê¸ˆë‹¨ê°€ ê³„ì‚° í•¨ìˆ˜
RATE_TABLE = {
    "before": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4},
    },
    "after": {
        "ì—¬ë¦„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
        "ë´„ê°€ì„": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
        "ê²¨ìš¸": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3},
    },
}

CUTOFF = datetime(2024, 10, 24)

def get_season(month):
    return "ì—¬ë¦„" if month in [6, 7, 8] else "ë´„ê°€ì„" if month in [3, 4, 5, 9, 10] else "ê²¨ìš¸"
def get_time_zone(hour, season):
    if season in ["ì—¬ë¦„", "ë´„ê°€ì„"]:
        if 22 <= hour or hour < 8: return "ê²½ë¶€í•˜"
        if (8 <= hour < 11) or (12 <= hour < 13) or (18 <= hour < 22): return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8: return "ê²½ë¶€í•˜"
        if (8 <= hour < 9) or (12 <= hour < 16) or (19 <= hour < 22): return "ì¤‘ê°„ë¶€í•˜"
        return "ìµœëŒ€ë¶€í•˜"

for df in [train_df, test_df]:
    df["ê³„ì ˆ"] = df["ì›”"].apply(get_season)
    df["ì ìš©ì‹œì "] = df["ì¸¡ì •ì¼ì‹œ"].apply(lambda x: "before" if x < CUTOFF else "after")
    df["ì‹œê°„ëŒ€"] = df.apply(lambda r: get_time_zone(r["ì‹œê°„"], r["ê³„ì ˆ"]), axis=1)
    df["ìš”ê¸ˆë‹¨ê°€"] = df.apply(lambda r: RATE_TABLE[r["ì ìš©ì‹œì "]][r["ê³„ì ˆ"]][r["ì‹œê°„ëŒ€"]], axis=1)

# ğŸ”¡ 4. ì¸ì½”ë”©
le = LabelEncoder()
train_df["ì‘ì—…ìœ í˜•_encoded"] = le.fit_transform(train_df["ì‘ì—…ìœ í˜•"])
test_df["ì‘ì—…ìœ í˜•_encoded"] = le.transform(test_df["ì‘ì—…ìœ í˜•"])

def target_encoding(df_train, df_test, col, target):
    global_mean = df_train[target].mean()
    agg = df_train.groupby(col)[target].agg(["mean", "count"])
    weight = 1 / (1 + np.exp(-(agg["count"] - 10)))
    enc = global_mean * (1 - weight) + agg["mean"] * weight
    mapping = enc.to_dict()
    df_train[f"{col}_te"] = df_train[col].map(mapping)
    df_test[f"{col}_te"] = df_test[col].map(mapping)

for col in ["ì‘ì—…ìœ í˜•", "ì‹œê°„", "ìš”ì¼", "ì‹œê°„ëŒ€"]:
    target_encoding(train_df, test_df, col, "ì „ê¸°ìš”ê¸ˆ(ì›)")

# ğŸ“ 5. ì…ë ¥ ë³€ìˆ˜ì™€ ì‹œí€€ìŠ¤ ì •ì˜
FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded", "ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "ì£¼ë§ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„", "ìš”ê¸ˆë‹¨ê°€",
    "ì‘ì—…ìœ í˜•_te", "ì‹œê°„_te", "ìš”ì¼_te", "ì‹œê°„ëŒ€_te"
]
TARGETS = ["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)", "íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)", "ì „ê¸°ìš”ê¸ˆ(ì›)"]
TIME_STEPS = 96 * 7

# ğŸ”„ 6. ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ë° ì •ê·œí™”
with open("./pickles/seq_scaler.pkl", "rb") as f:
    seq_scaler = pickle.load(f)

full_input = pd.concat([
    pd.concat([train_df[FEATURES], train_df[TARGETS]], axis=1),
    pd.concat([test_df[FEATURES], pd.DataFrame(0, index=range(len(test_df)), columns=TARGETS)], axis=1)
], ignore_index=True)

full_scaled = seq_scaler.transform(full_input)
scaled_df = pd.DataFrame(full_scaled, columns=FEATURES + TARGETS)

# ğŸ§  7. LSTM ë¡œë“œ ë° ìŠ¬ë¼ì´ë”© ì˜ˆì¸¡
model = load_model("./pickles/lstm_multi_output.h5", compile=False)
pred_list = []

last_sequence = scaled_df.iloc[-(TIME_STEPS + len(test_df)):-len(test_df)][FEATURES].values.copy()

for i in range(len(test_df)):
    input_seq = last_sequence[-TIME_STEPS:]
    pred_scaled = model.predict(input_seq[np.newaxis, :, :], verbose=0)

    dummy_input = np.zeros((1, len(FEATURES)))
    full_row = np.concatenate([dummy_input, pred_scaled], axis=1)
    inverse_row = seq_scaler.inverse_transform(full_row)[0][-len(TARGETS):]

    pred_list.append(inverse_row)

    # ìŠ¬ë¼ì´ë”© ì ìš©: ë‹¤ìŒ ì…ë ¥ ì—…ë°ì´íŠ¸
    next_input = scaled_df.iloc[-len(test_df) + i][FEATURES].values  # ë‚ ì”¨ ë“±ì€ ì‚¬ìš©
    last_sequence = np.vstack([last_sequence[1:], next_input])

# ğŸ“¤ 8. ê²°ê³¼ ì €ì¥
pred_df = pd.DataFrame(pred_list, columns=TARGETS)
submission = pd.DataFrame({
    "id": test_df["id"],
    "ì „ê¸°ìš”ê¸ˆ(ì›)": pred_df["ì „ê¸°ìš”ê¸ˆ(ì›)"]
})
submission.to_csv("submission_lstm.csv", index=False)
print("âœ… ìŠ¬ë¼ì´ë”© ì˜ˆì¸¡ ì™„ë£Œ â†’ submission_lstm.csv ì €ì¥ë¨")


# -----------------------------------------------------------------
import pandas as pd
import numpy as np
import pickle
from tensorflow.keras.models import load_model

# ğŸ“‚ ë°ì´í„° ë¡œë“œ ë° ì‹œê³„ì—´ scaler ë¡œë”©
test_df = pd.read_csv("./data/test_with_weather.csv")
with open("./pickles/seq_scaler.pkl", "rb") as f:
    seq_scaler = pickle.load(f)

# ğŸ“„ í”¼ì²˜ ì„¤ì •
FEATURES = [
    "ì‘ì—…ìœ í˜•_encoded", "ì›”", "ì¼", "ì‹œê°„", "ìš”ì¼", "ì£¼ë§ì—¬ë¶€",
    "sin_ì‹œê°„", "cos_ì‹œê°„", "ìš”ê¸ˆë‹¨ê°€",
    "ì‘ì—…ìœ í˜•_te", "ì‹œê°„_te", "ìš”ì¼_te", "ì‹œê°„ëŒ€_te"
]
TARGETS = ["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)", "íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)", "ì „ê¸°ìš”ê¸ˆ(ì›)"]
TIME_STEPS = 96 * 7

# ì „ì²´ ì‹œí€€ìŠ¤ ë°ì´í„° ì¬êµ¬ì„±
full_input = pd.read_csv("./data/full_scaled.csv")  # ë¯¸ë¦¬ scalingëœ ì „ì²´ ì‹œí€€ìŠ¤ë¥¼ ì €ì¥í•œ íŒŒì¼
full_scaled = full_input.values
scaled_df = pd.DataFrame(full_scaled, columns=FEATURES + TARGETS)

# ìŠ¬ë¼ì´ë”© ì˜ˆì¸¡ ì‹œì‘ì 
X_input = scaled_df.iloc[-TIME_STEPS:][FEATURES].values
X_pred = []

model = load_model("/mnt/data/lstm_multi_output.h5", compile=False)

for _ in range(len(test_df)):
    x = np.expand_dims(X_input, axis=0)  # (1, TIME_STEPS, features)
    pred_scaled = model.predict(x, verbose=0)[0]

    # ì˜ˆì¸¡ê°’ì„ ì—­ì •ê·œí™” ì—†ì´ ë‹¤ì‹œ ë¶™ì´ê¸° ìœ„í•´ ìŠ¤ì¼€ì¼ëœ ìƒíƒœë¡œ ìœ ì§€
    next_row = np.concatenate([X_input[-1], pred_scaled])  # (features + targets,)
    full_scaled_next = np.append(X_input[1:], [next_row[:len(FEATURES)]], axis=0)
    X_input = full_scaled_next

    X_pred.append(pred_scaled)

# ê²°ê³¼ ì—­ì •ê·œí™”
X_pred = np.array(X_pred)
dummy = np.zeros((len(X_pred), len(FEATURES)))
recon = np.concatenate([dummy, X_pred], axis=1)
inv_preds = seq_scaler.inverse_transform(recon)[:, -len(TARGETS):]
pred_df = pd.DataFrame(inv_preds, columns=TARGETS)
pred_df["id"] = test_df["id"]

# ì „ê¸°ìš”ê¸ˆ ì˜ˆì¸¡ë§Œ ì €ì¥
submission = pred_df[["id", "ì „ê¸°ìš”ê¸ˆ(ì›)"]]
submission.to_csv("./data/submission_lstm_sliding.csv", index=False)




