import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams['font.family'] = 'Malgun Gothic'
# ë°ì´í„° ë¡œë”©
train_df = pd.read_csv("./data/train.csv")  # ê²½ë¡œëŠ” ì‹¤ì œ íŒŒì¼ ìœ„ì¹˜ì— ë§ê²Œ ì¡°ì •
train_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(train_df['ì¸¡ì •ì¼ì‹œ'])

# íŒŒìƒ ë³€ìˆ˜ ìƒì„±
train_df['ìš”ì¼'] = train_df['ì¸¡ì •ì¼ì‹œ'].dt.day_name()
train_df['ì‹œê°„'] = train_df['ì¸¡ì •ì¼ì‹œ'].dt.hour
train_df['ì›”'] = train_df['ì¸¡ì •ì¼ì‹œ'].dt.month

# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
numeric_cols = [
    'ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)',
    'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)', 'ì „ê¸°ìš”ê¸ˆ(ì›)'
]
train_df.columns
# ìš”ì¼ ìˆœì„œ ì§€ì •
weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']

# ì‹œê°í™” í•¨ìˆ˜ ì •ì˜
def plot_comparison_by_timeunit(df, col, time_unit, order=None):
    plt.figure(figsize=(12, 5))
    sns.boxplot(data=df, x=time_unit, y=col, order=order)
    plt.title(f"{time_unit}ë³„ {col} ë¶„í¬")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

# ì‹œê°í™” ì‹¤í–‰
for col in numeric_cols:
    plot_comparison_by_timeunit(train_df, col, 'ìš”ì¼', order=weekday_order)
    plot_comparison_by_timeunit(train_df, col, 'ì‹œê°„')
    plot_comparison_by_timeunit(train_df, col, 'ì›”')


import seaborn as sns
import matplotlib.pyplot as plt

# ë™ì¼í•œ ì „ë ¥ì‚¬ìš©ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ ìš”ê¸ˆì˜ ì›”ë³„ ì°¨ì´ í™•ì¸
plt.figure(figsize=(12, 6))
sns.scatterplot(
    data=train_df,
    x='ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)',
    y='ì „ê¸°ìš”ê¸ˆ(ì›)',
    hue='ì›”',
    palette='tab10',
    alpha=0.6
)
plt.title("ì „ë ¥ì‚¬ìš©ëŸ‰(kWh) vs ì „ê¸°ìš”ê¸ˆ(ì›) (ì›”ë³„ êµ¬ë¶„)")
plt.tight_layout()
plt.show()



# ìš”ê¸ˆì œë³„ ë‹¨ê°€ ì •ì˜: [ê³ ì••A/B], [ì„ íƒâ… /â…¡], [2024.01.01, 2024.10.24]
# êµ¬ì¡°: (ìš”ê¸ˆì œëª…, ë‚ ì§œêµ¬ê°„, ë¶€í•˜ â†’ ê³„ì ˆ â†’ ë‹¨ê°€)
import numpy as np
from datetime import datetime

# ë‚ ì§œ êµ¬ê°„ ë‚˜ëˆ„ê¸°
before_change = train_df['ì¸¡ì •ì¼ì‹œ'] < pd.to_datetime("2024-10-24")
after_change = ~before_change

# ì‹œê°„ëŒ€ ë¶„ë¥˜ í•¨ìˆ˜ëŠ” ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆìŒ: get_time_block

# ìš”ê¸ˆí‘œ (ê³ ì••A/B ì„ íƒ1,2 ê°ê° 2ì‹œê¸°)
tariffs = {
    "ê³ ì••A_ì„ íƒ1_20240101": {
        "summer": {"ê²½ë¶€í•˜": 99.5, "ì¤‘ê°„ë¶€í•˜": 152.4, "ìµœëŒ€ë¶€í•˜": 234.5},
        "spring_fall": {"ê²½ë¶€í•˜": 99.5, "ì¤‘ê°„ë¶€í•˜": 122.0, "ìµœëŒ€ë¶€í•˜": 152.7},
        "winter": {"ê²½ë¶€í•˜": 106.5, "ì¤‘ê°„ë¶€í•˜": 152.6, "ìµœëŒ€ë¶€í•˜": 210.1},
    },
    "ê³ ì••A_ì„ íƒ1_20241024": {
        "summer": {"ê²½ë¶€í•˜": 116.4, "ì¤‘ê°„ë¶€í•˜": 169.3, "ìµœëŒ€ë¶€í•˜": 251.4},
        "spring_fall": {"ê²½ë¶€í•˜": 116.4, "ì¤‘ê°„ë¶€í•˜": 138.9, "ìµœëŒ€ë¶€í•˜": 169.6},
        "winter": {"ê²½ë¶€í•˜": 123.4, "ì¤‘ê°„ë¶€í•˜": 169.5, "ìµœëŒ€ë¶€í•˜": 227.0},
    },
    "ê³ ì••A_ì„ íƒ2_20240101": {
        "summer": {"ê²½ë¶€í•˜": 94.0, "ì¤‘ê°„ë¶€í•˜": 146.9, "ìµœëŒ€ë¶€í•˜": 229.0},
        "spring_fall": {"ê²½ë¶€í•˜": 94.0, "ì¤‘ê°„ë¶€í•˜": 116.5, "ìµœëŒ€ë¶€í•˜": 147.2},
        "winter": {"ê²½ë¶€í•˜": 101.0, "ì¤‘ê°„ë¶€í•˜": 147.1, "ìµœëŒ€ë¶€í•˜": 204.6},
    },
    "ê³ ì••A_ì„ íƒ2_20241024": {
        "summer": {"ê²½ë¶€í•˜": 110.9, "ì¤‘ê°„ë¶€í•˜": 163.8, "ìµœëŒ€ë¶€í•˜": 245.9},
        "spring_fall": {"ê²½ë¶€í•˜": 110.9, "ì¤‘ê°„ë¶€í•˜": 133.4, "ìµœëŒ€ë¶€í•˜": 164.1},
        "winter": {"ê²½ë¶€í•˜": 117.9, "ì¤‘ê°„ë¶€í•˜": 164.0, "ìµœëŒ€ë¶€í•˜": 221.5},
    },
    "ê³ ì••B_ì„ íƒ1_20240101": {
        "summer": {"ê²½ë¶€í•˜": 109.4, "ì¤‘ê°„ë¶€í•˜": 161.7, "ìµœëŒ€ë¶€í•˜": 242.9},
        "spring_fall": {"ê²½ë¶€í•˜": 109.4, "ì¤‘ê°„ë¶€í•˜": 131.7, "ìµœëŒ€ë¶€í•˜": 162.0},
        "winter": {"ê²½ë¶€í•˜": 116.4, "ì¤‘ê°„ë¶€í•˜": 161.7, "ìµœëŒ€ë¶€í•˜": 217.9},
    },
    "ê³ ì••B_ì„ íƒ1_20241024": {
        "summer": {"ê²½ë¶€í•˜": 126.3, "ì¤‘ê°„ë¶€í•˜": 178.6, "ìµœëŒ€ë¶€í•˜": 259.8},
        "spring_fall": {"ê²½ë¶€í•˜": 126.3, "ì¤‘ê°„ë¶€í•˜": 148.6, "ìµœëŒ€ë¶€í•˜": 178.9},
        "winter": {"ê²½ë¶€í•˜": 133.3, "ì¤‘ê°„ë¶€í•˜": 178.6, "ìµœëŒ€ë¶€í•˜": 234.8},
    },
    "ê³ ì••B_ì„ íƒ2_20240101": {
        "summer": {"ê²½ë¶€í•˜": 105.6, "ì¤‘ê°„ë¶€í•˜": 157.9, "ìµœëŒ€ë¶€í•˜": 239.1},
        "spring_fall": {"ê²½ë¶€í•˜": 105.6, "ì¤‘ê°„ë¶€í•˜": 127.9, "ìµœëŒ€ë¶€í•˜": 158.2},
        "winter": {"ê²½ë¶€í•˜": 112.6, "ì¤‘ê°„ë¶€í•˜": 157.9, "ìµœëŒ€ë¶€í•˜": 214.1},
    },
    "ê³ ì••B_ì„ íƒ2_20241024": {
        "summer": {"ê²½ë¶€í•˜": 122.5, "ì¤‘ê°„ë¶€í•˜": 174.8, "ìµœëŒ€ë¶€í•˜": 256.0},
        "spring_fall": {"ê²½ë¶€í•˜": 122.5, "ì¤‘ê°„ë¶€í•˜": 144.8, "ìµœëŒ€ë¶€í•˜": 175.1},
        "winter": {"ê²½ë¶€í•˜": 129.5, "ì¤‘ê°„ë¶€í•˜": 174.8, "ìµœëŒ€ë¶€í•˜": 231.0},
    }
}

# ì˜¤ì°¨ ê³„ì‚° í•¨ìˆ˜
def estimate_tariff_error(df, rate_set):
    est_fees = []
    for _, row in df.iterrows():
        hour = row['ì‹œê°„']
        month = row['ì›”']
        usage = row['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)']
        time_block, season_key = get_time_block(hour, month)
        rate = rate_set[season_key][time_block]
        est_fees.append(usage * rate)
    return np.mean(np.abs(df['ì „ê¸°ìš”ê¸ˆ(ì›)'] - est_fees))

# ì „ì²´ ìš”ê¸ˆì œ í‰ê·  ì˜¤ì°¨ ë¹„êµ
results = {}
for name, rate_set in tariffs.items():
    if "20240101" in name:
        df_sub = train_df[before_change]
    else:
        df_sub = train_df[after_change]

    error = estimate_tariff_error(df_sub, rate_set)
    results[name] = error

# ê°€ì¥ ì˜¤ì°¨ê°€ ì‘ì€ ìš”ê¸ˆì œ ì •ë ¬
sorted_results = dict(sorted(results.items(), key=lambda x: x[1]))
sorted_results


pd.read_csv("./data/test.csv")





import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error
import holidays

# ë³µì‚¬ ë° ê¸°ë³¸ íŒŒìƒë³€ìˆ˜ ìƒì„±
df = train_df.copy()
df['ì›”'] = df['ì¸¡ì •ì¼ì‹œ'].dt.month
df['ìš”ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.dayofweek
df['hour'] = df['ì¸¡ì •ì¼ì‹œ'].dt.hour
kr_holidays = holidays.KR(years=[2024])
df['íœ´ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.date.apply(lambda x: x in kr_holidays or pd.to_datetime(x).weekday() >= 5)

# ê³„ì ˆ/ì‹œê°„ëŒ€ ë¶„ë¥˜ í•¨ìˆ˜
def get_time_block(hour, month):
    if month in [6, 7, 8]:
        season = "summer"
    elif month in [3, 4, 5, 9, 10]:
        season = "spring_fall"
    else:
        season = "winter"

    if season in ["summer", "spring_fall"]:
        if 22 <= hour or hour < 8:
            block = "ê²½ë¶€í•˜"
        elif 8 <= hour < 11 or 12 <= hour < 13 or 18 <= hour < 22:
            block = "ì¤‘ê°„ë¶€í•˜"
        else:
            block = "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8:
            block = "ê²½ë¶€í•˜"
        elif 8 <= hour < 9 or 12 <= hour < 16 or 19 <= hour < 22:
            block = "ì¤‘ê°„ë¶€í•˜"
        else:
            block = "ìµœëŒ€ë¶€í•˜"
    return block, season

# ê³ ì••A ì„ íƒ3 ìš”ê¸ˆ ë‹¨ê°€
tariff_a3_20240101 = {
    "summer": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
    "spring_fall": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
    "winter": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4}
}

tariff_a3_20241024 = {
    "summer": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
    "spring_fall": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
    "winter": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3}
}

# ìš”ê¸ˆì œ ë‹¨ê°€ íŒŒìƒë³€ìˆ˜ ìƒì„±
def estimate_a3_fee_v2(row):
    block, season = get_time_block(row['hour'], row['ì›”'])
    ê¸°ì¤€ì¼ = pd.to_datetime("2024-10-24")
    if row['ì¸¡ì •ì¼ì‹œ'] < ê¸°ì¤€ì¼:
        rate = tariff_a3_20240101[season][block]
    else:
        rate = tariff_a3_20241024[season][block]
    return rate

df['ìš”ê¸ˆì œ'] = df.apply(estimate_a3_fee_v2, axis=1)

# í•™ìŠµ/í‰ê°€ ë°ì´í„° ë¶„ë¦¬ (6ì›” ì œì™¸ í•™ìŠµ, 6ì›” í…ŒìŠ¤íŠ¸)
train_data = df[df['ì›”'].isin([1, 2, 3, 4, 5, 7, 8, 9, 10, 11])]
test_data = df[df['ì›”'] == 6]

X_train = train_data[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'hour', 'ì‘ì—…ìœ í˜•', 'ìš”ê¸ˆì œ']]
y_train = train_data['ì „ê¸°ìš”ê¸ˆ(ì›)']
X_test = test_data[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'hour', 'ì‘ì—…ìœ í˜•', 'ìš”ê¸ˆì œ']]
y_test = test_data['ì „ê¸°ìš”ê¸ˆ(ì›)']

# ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì •ì˜
categorical_features = ['ìš”ì¼', 'íœ´ì¼', 'ì‘ì—…ìœ í˜•']
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),
    ],
    remainder='passthrough'
)

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42)
}

# í•™ìŠµ ë° í‰ê°€
final_results = {}
for name, model in models.items():
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    final_results[name] = mse

final_results





import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error
import holidays

train_df = pd.read_csv("./data/train.csv")

# ì›ë³¸ ë°ì´í„° ë³µì‚¬
df = train_df.copy()
df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(df['ì¸¡ì •ì¼ì‹œ'])
# ê¸°ë³¸ íŒŒìƒë³€ìˆ˜ ìƒì„±
df['ì›”'] = df['ì¸¡ì •ì¼ì‹œ'].dt.month
df['ìš”ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.dayofweek
df['hour'] = df['ì¸¡ì •ì¼ì‹œ'].dt.hour
kr_holidays = holidays.KR(years=[2024])
df['íœ´ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.date.apply(lambda x: x in kr_holidays or pd.to_datetime(x).weekday() >= 5)

# ì‹œê°„ëŒ€-ê³„ì ˆë³„ êµ¬ë¶„ í•¨ìˆ˜
def get_time_block(hour, month):
    if month in [6, 7, 8]:
        season = "summer"
    elif month in [3, 4, 5, 9, 10]:
        season = "spring_fall"
    else:
        season = "winter"
    if season in ["summer", "spring_fall"]:
        if 22 <= hour or hour < 8:
            block = "ê²½ë¶€í•˜"
        elif 8 <= hour < 11 or 12 <= hour < 13 or 18 <= hour < 22:
            block = "ì¤‘ê°„ë¶€í•˜"
        else:
            block = "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8:
            block = "ê²½ë¶€í•˜"
        elif 8 <= hour < 9 or 12 <= hour < 16 or 19 <= hour < 22:
            block = "ì¤‘ê°„ë¶€í•˜"
        else:
            block = "ìµœëŒ€ë¶€í•˜"
    return block, season

# ê³ ì••A ì„ íƒ3 ìš”ê¸ˆì œ
tariff_a3_20240101 = {
    "summer": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
    "spring_fall": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
    "winter": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4}
}
tariff_a3_20241024 = {
    "summer": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
    "spring_fall": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
    "winter": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3}
}

def estimate_a3_fee(row):
    block, season = get_time_block(row['hour'], row['ì›”'])
    ê¸°ì¤€ì¼ = pd.to_datetime("2024-10-24")
    rate = tariff_a3_20240101[season][block] if row['ì¸¡ì •ì¼ì‹œ'] < ê¸°ì¤€ì¼ else tariff_a3_20241024[season][block]
    return rate

df['ìš”ê¸ˆì œ'] = df.apply(estimate_a3_fee, axis=1)

# 6ì›”ë§Œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬, ë‚˜ë¨¸ì§€ëŠ” í•™ìŠµ
train_data = df[df['ì›”'].isin([1, 2, 3, 4, 5, 7, 8, 9, 10, 11])]
test_data = df[df['ì›”'] == 6]

X_train = train_data[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'hour', 'ì‘ì—…ìœ í˜•', 'ìš”ê¸ˆì œ']]
y_train = train_data['ì „ê¸°ìš”ê¸ˆ(ì›)']
X_test = test_data[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'hour', 'ì‘ì—…ìœ í˜•', 'ìš”ê¸ˆì œ']]
y_test = test_data['ì „ê¸°ìš”ê¸ˆ(ì›)']

# ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì •ì˜
categorical_features = ['ìš”ì¼', 'íœ´ì¼', 'ì‘ì—…ìœ í˜•']
preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
], remainder='passthrough')

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42)
}

# MAEë¡œ í‰ê°€
mae_results = {}
for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    mae_results[name] = mae

mae_results














import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error
import holidays

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train_df = pd.read_csv("./data/train.csv")
test_df = pd.read_csv("./data/test.csv")

# ë‚ ì§œ ë³€í™˜
train_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(train_df['ì¸¡ì •ì¼ì‹œ'])
test_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(test_df['ì¸¡ì •ì¼ì‹œ'])

# íŒŒìƒë³€ìˆ˜ ìƒì„± í•¨ìˆ˜
def create_features(df):
    df['ì›”'] = df['ì¸¡ì •ì¼ì‹œ'].dt.month
    df['ìš”ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.dayofweek
    kr_holidays = holidays.KR(years=[2024])
    df['íœ´ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.date.apply(lambda x: x in kr_holidays or pd.to_datetime(x).weekday() >= 5)
    return df

train_df = create_features(train_df)
test_df = create_features(test_df)

# ë¶€í•˜ ìš”ê¸ˆí‘œ (ì‘ì—…ìœ í˜•ì„ ë¶€í•˜ë¡œ ê°„ì£¼)
tariff_map_20240101 = {
    "Light_Load": {"summer": 93.1, "spring_fall": 93.1, "winter": 100.4},
    "Medium_Load": {"summer": 146.3, "spring_fall": 115.2, "winter": 146.5},
    "Maximum_Load": {"summer": 216.6, "spring_fall": 138.9, "winter": 193.4}
}
tariff_map_20241024 = {
    "Light_Load": {"summer": 110.0, "spring_fall": 110.0, "winter": 117.3},
    "Medium_Load": {"summer": 163.2, "spring_fall": 132.1, "winter": 163.4},
    "Maximum_Load": {"summer": 233.5, "spring_fall": 155.8, "winter": 210.3}
}

# ê³„ì ˆ êµ¬ë¶„
def get_season(month):
    if month in [6, 7, 8]:
        return "summer"
    elif month in [3, 4, 5, 9, 10]:
        return "spring_fall"
    else:
        return "winter"

# ìš”ê¸ˆì œ ê³„ì‚° í•¨ìˆ˜
def compute_tariff(row):
    ê¸°ì¤€ì¼ = pd.to_datetime("2024-10-24")
    season = get_season(row['ì›”'])
    if row['ì¸¡ì •ì¼ì‹œ'] < ê¸°ì¤€ì¼:
        return tariff_map_20240101[row['ì‘ì—…ìœ í˜•']][season]
    else:
        return tariff_map_20241024[row['ì‘ì—…ìœ í˜•']][season]

# ìš”ê¸ˆì œ ì ìš©
train_df['ìš”ê¸ˆì œ'] = train_df.apply(compute_tariff, axis=1)
test_df['ìš”ê¸ˆì œ'] = test_df.apply(compute_tariff, axis=1)

# ìµœì¢… ëª¨ë¸ feature ì§€ì • (ì‘ì—…ìœ í˜• ì œê±°ë¨)
X_train = train_df[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'ìš”ê¸ˆì œ']]
y_train = train_df['ì „ê¸°ìš”ê¸ˆ(ì›)']
X_test = test_df[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'ìš”ê¸ˆì œ']]

# ì „ì²˜ë¦¬ ë° ëª¨ë¸ ì •ì˜
categorical_features = ['ìš”ì¼', 'íœ´ì¼']
preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
], remainder='passthrough')

models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42)
}

# ì˜ˆì¸¡ ìˆ˜í–‰
predictions = {}
for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    pipeline.fit(X_train, y_train)
    preds = pipeline.predict(X_test)
    predictions[name] = preds

# ìµœì¢… ê²°ê³¼ ì €ì¥ (XGBoost ê¸°ì¤€)
test_df['ì˜ˆì¸¡_ì „ê¸°ìš”ê¸ˆ(ì›)_XGB'] = predictions['XGBoost']
submission = test_df[['id', 'ì˜ˆì¸¡_ì „ê¸°ìš”ê¸ˆ(ì›)_XGB']].copy()
submission.columns = ['id', 'ì „ê¸°ìš”ê¸ˆ(ì›)']
submission.to_csv("./submission.csv", index=False)








import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error
import holidays

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
train_df = pd.read_csv("./data/train.csv")
train_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(train_df['ì¸¡ì •ì¼ì‹œ'])

# íŒŒìƒë³€ìˆ˜ ìƒì„±
def create_features(df):
    df['ì›”'] = df['ì¸¡ì •ì¼ì‹œ'].dt.month
    df['ìš”ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.dayofweek
    kr_holidays = holidays.KR(years=[2024])
    df['íœ´ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.date.apply(lambda x: x in kr_holidays or pd.to_datetime(x).weekday() >= 5)
    return df

train_df = create_features(train_df)

# ê³ ì••A ì„ íƒ3 ìš”ê¸ˆì œ ë‹¨ê°€í‘œ
tariff_map_20240101 = {
    "Light_Load": {"summer": 93.1, "spring_fall": 93.1, "winter": 100.4},
    "Medium_Load": {"summer": 146.3, "spring_fall": 115.2, "winter": 146.5},
    "Maximum_Load": {"summer": 216.6, "spring_fall": 138.9, "winter": 193.4}
}
tariff_map_20241024 = {
    "Light_Load": {"summer": 110.0, "spring_fall": 110.0, "winter": 117.3},
    "Medium_Load": {"summer": 163.2, "spring_fall": 132.1, "winter": 163.4},
    "Maximum_Load": {"summer": 233.5, "spring_fall": 155.8, "winter": 210.3}
}

# ê³„ì ˆ êµ¬ë¶„
def get_season(month):
    if month in [6, 7, 8]:
        return "summer"
    elif month in [3, 4, 5, 9, 10]:
        return "spring_fall"
    else:
        return "winter"

# ìš”ê¸ˆì œ ê³„ì‚° í•¨ìˆ˜
def compute_tariff(row):
    ê¸°ì¤€ì¼ = pd.to_datetime("2024-10-24")
    season = get_season(row['ì›”'])
    if row['ì¸¡ì •ì¼ì‹œ'] < ê¸°ì¤€ì¼:
        return tariff_map_20240101[row['ì‘ì—…ìœ í˜•']][season]
    else:
        return tariff_map_20241024[row['ì‘ì—…ìœ í˜•']][season]

# ìš”ê¸ˆì œ ì ìš©
train_df['ìš”ê¸ˆì œ'] = train_df.apply(compute_tariff, axis=1)

# 6ì›”ì„ í…ŒìŠ¤íŠ¸ë¡œ ë¶„ë¦¬
train_data = train_df[train_df['ì›”'] != 6]
test_data = train_df[train_df['ì›”'] == 6]

X_train = train_data[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'ìš”ê¸ˆì œ']]
y_train = train_data['ì „ê¸°ìš”ê¸ˆ(ì›)']
X_test = test_data[['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'ìš”ê¸ˆì œ']]
y_test = test_data['ì „ê¸°ìš”ê¸ˆ(ì›)']

# ì „ì²˜ë¦¬
categorical_features = ['ìš”ì¼', 'íœ´ì¼']
preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
], remainder='passthrough')

# ëª¨ë¸ ì •ì˜
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(random_state=42),
    "XGBoost": XGBRegressor(random_state=42),
    "LightGBM": LGBMRegressor(random_state=42)
}

# MAE í‰ê°€
mae_results = {}
for name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    pipeline.fit(X_train, y_train)
    preds = pipeline.predict(X_test)
    mae = mean_absolute_error(y_test, preds)
    mae_results[name] = mae
    print(f"{name} â†’ MAE: {mae:.2f}")

# ìµœì¢… ê²°ê³¼ ì¶œë ¥
print("\nâœ… ì „ì²´ MAE ê²°ê³¼:")
print(mae_results)



import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.callbacks import EarlyStopping

# -----------------------
# 1. Load & Sort
# -----------------------
train_df = pd.read_csv("./data/train.csv")
test_df = pd.read_csv("./data/test.csv")

train_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(train_df['ì¸¡ì •ì¼ì‹œ'])
test_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(test_df['ì¸¡ì •ì¼ì‹œ'])

train_df = train_df.sort_values("ì¸¡ì •ì¼ì‹œ").reset_index(drop=True)
test_df = test_df.sort_values("ì¸¡ì •ì¼ì‹œ").reset_index(drop=True)

# -----------------------
# 2. Create Time Features
# -----------------------
def create_time_features(df):
    df['hour'] = df['ì¸¡ì •ì¼ì‹œ'].dt.hour
    df['minute'] = df['ì¸¡ì •ì¼ì‹œ'].dt.minute
    df['dayofweek'] = df['ì¸¡ì •ì¼ì‹œ'].dt.dayofweek
    return df

train_df = create_time_features(train_df)
test_df = create_time_features(test_df)

# -----------------------
# 3. Combine last 24 rows of train with test
# -----------------------
window_size = 24
combined_df = pd.concat([train_df.tail(window_size), test_df], ignore_index=True)

# -----------------------
# 4. Select Features and Normalize
# -----------------------
feature_cols = ['hour', 'minute', 'dayofweek']
target_cols = ['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)',
               'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)']

X_all_raw = combined_df[feature_cols].values
y_train_raw = train_df[target_cols].values  # Only training labels

# Scaling
X_scaler = MinMaxScaler()
y_scaler = MinMaxScaler()

X_all_scaled = X_scaler.fit_transform(X_all_raw)
y_train_scaled = y_scaler.fit_transform(y_train_raw)

# -----------------------
# 5. Create Sequences
# -----------------------
def create_sequences(X, y=None, window_size=24):
    Xs, ys = [], []
    for i in range(len(X) - window_size):
        Xs.append(X[i:i + window_size])
        if y is not None:
            ys.append(y[i + window_size])
    return np.array(Xs), np.array(ys) if y is not None else np.array(Xs)

# í•™ìŠµ ë°ì´í„°: ì „ì²´ ì‹œí€€ìŠ¤ì—ì„œ train ê¸¸ì´ë§Œí¼ë§Œ ì‚¬ìš©
X_seq_all, y_seq_all = create_sequences(X_all_scaled, None, window_size)
X_seq_train = X_seq_all[:len(train_df) - window_size]
y_seq_train = y_train_scaled[window_size:]

# í…ŒìŠ¤íŠ¸ ë°ì´í„°: ë§ˆì§€ë§‰ test ê¸¸ì´ë§Œí¼ ì‹œí€€ìŠ¤ ìë¥´ê¸°
X_seq_test = X_seq_all[-len(test_df):]

# -----------------------
# 6. LSTM Model
# -----------------------
model = Sequential([
    LSTM(64, input_shape=(window_size, X_seq_train.shape[2]), return_sequences=False),
    Dense(32, activation='relu'),
    Dense(len(target_cols))  # 5ê°œ ì¶œë ¥
])

model.compile(optimizer='adam', loss='mse')
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_seq_train, y_seq_train, validation_split=0.2,
          epochs=50, batch_size=32, callbacks=[early_stop], verbose=1)

# -----------------------
# 7. Predict and Inverse Transform
# -----------------------
y_pred_scaled = model.predict(X_seq_test)
y_pred = y_scaler.inverse_transform(y_pred_scaled)

# -----------------------
# 8. Save Predictions
# -----------------------
result_df = test_df.copy()
result_df[target_cols] = y_pred
result_df.to_csv("test_predicted.csv", index=False)

print(result_df[target_cols].head())


import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# Load data
train_df = pd.read_csv("./data/train.csv")
test_df = pd.read_csv("./test_predicted.csv")
train_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(train_df['ì¸¡ì •ì¼ì‹œ'])
test_df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(test_df['ì¸¡ì •ì¼ì‹œ'])

# Feature engineering
def create_features(df):
    df['ì›”'] = df['ì¸¡ì •ì¼ì‹œ'].dt.month
    df['ìš”ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.dayofweek
    df['hour'] = df['ì¸¡ì •ì¼ì‹œ'].dt.hour
    df['íœ´ì¼'] = df['ìš”ì¼'].isin([0, 6])  # Monday, Sunday = íœ´ì¼
    return df

train_df = create_features(train_df)
test_df = create_features(test_df)

# ìš”ê¸ˆì œ ê³„ì‚° í•¨ìˆ˜
def get_time_block(hour, month):
    if month in [6, 7, 8]: season = "summer"
    elif month in [3, 4, 5, 9, 10]: season = "spring_fall"
    else: season = "winter"
    if season in ["summer", "spring_fall"]:
        if 22 <= hour or hour < 8: block = "ê²½ë¶€í•˜"
        elif 8 <= hour < 11 or 12 <= hour < 13 or 18 <= hour < 22: block = "ì¤‘ê°„ë¶€í•˜"
        else: block = "ìµœëŒ€ë¶€í•˜"
    else:
        if 22 <= hour or hour < 8: block = "ê²½ë¶€í•˜"
        elif 8 <= hour < 9 or 12 <= hour < 16 or 19 <= hour < 22: block = "ì¤‘ê°„ë¶€í•˜"
        else: block = "ìµœëŒ€ë¶€í•˜"
    return block, season

tariff_a3_20240101 = {
    "summer": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 146.3, "ìµœëŒ€ë¶€í•˜": 216.6},
    "spring_fall": {"ê²½ë¶€í•˜": 93.1, "ì¤‘ê°„ë¶€í•˜": 115.2, "ìµœëŒ€ë¶€í•˜": 138.9},
    "winter": {"ê²½ë¶€í•˜": 100.4, "ì¤‘ê°„ë¶€í•˜": 146.5, "ìµœëŒ€ë¶€í•˜": 193.4}
}
tariff_a3_20241024 = {
    "summer": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 163.2, "ìµœëŒ€ë¶€í•˜": 233.5},
    "spring_fall": {"ê²½ë¶€í•˜": 110.0, "ì¤‘ê°„ë¶€í•˜": 132.1, "ìµœëŒ€ë¶€í•˜": 155.8},
    "winter": {"ê²½ë¶€í•˜": 117.3, "ì¤‘ê°„ë¶€í•˜": 163.4, "ìµœëŒ€ë¶€í•˜": 210.3}
}

def estimate_a3_fee(row):
    block, season = get_time_block(row['hour'], row['ì›”'])
    ê¸°ì¤€ì¼ = pd.to_datetime("2024-10-24")
    return tariff_a3_20240101[season][block] if row['ì¸¡ì •ì¼ì‹œ'] < ê¸°ì¤€ì¼ else tariff_a3_20241024[season][block]

train_df['ìš”ê¸ˆì œ'] = train_df.apply(estimate_a3_fee, axis=1)
test_df['ìš”ê¸ˆì œ'] = test_df.apply(estimate_a3_fee, axis=1)

# Features
lstm_cols = ['ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)']
feature_cols = ['ì›”', 'ìš”ì¼', 'íœ´ì¼', 'hour', 'ìš”ê¸ˆì œ'] + lstm_cols
X_train = train_df[feature_cols]
y_train = train_df['ì „ê¸°ìš”ê¸ˆ(ì›)']
X_test = test_df[feature_cols]

# Preprocessing
categorical_features = ['ìš”ì¼', 'íœ´ì¼']
preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
], remainder='passthrough')

# Model ì •ì˜ ë° íŒŒë¼ë¯¸í„° ì„¤ì •
models = {
    "Linear Regression": (LinearRegression(), {}),
    "Random Forest": (
        RandomForestRegressor(random_state=42),
        {
            'regressor__n_estimators': randint(100, 300),
            'regressor__max_depth': randint(3, 15)
        }
    ),
    "XGBoost": (
        XGBRegressor(random_state=42),
        {
            'regressor__n_estimators': randint(100, 300),
            'regressor__max_depth': randint(3, 15),
            'regressor__learning_rate': uniform(0.01, 0.2)
        }
    ),
    "LightGBM": (
        LGBMRegressor(random_state=42),
        {
            'regressor__n_estimators': randint(100, 300),
            'regressor__max_depth': randint(3, 15),
            'regressor__learning_rate': uniform(0.01, 0.2)
        }
    )
}

# MAE ì €ì¥
mae_results = {}
final_predictions = {}

# ëª¨ë¸ë³„ í•™ìŠµ ë° ì˜ˆì¸¡
for name, (model, param_dist) in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    if param_dist:
        search = RandomizedSearchCV(pipeline, param_distributions=param_dist,
                                    n_iter=20, cv=3, scoring='neg_mean_absolute_error',
                                    n_jobs=-1, random_state=42)
        search.fit(X_train, y_train)
        best_model = search.best_estimator_
    else:
        pipeline.fit(X_train, y_train)
        best_model = pipeline

    preds = best_model.predict(X_test)
    final_predictions[name] = preds

    # í‰ê°€ìš©ë„ - trainì„ ë‹¤ì‹œ ì˜ˆì¸¡í•´ì„œ MAE ì¸¡ì •
    train_pred = best_model.predict(X_train)
    mae = mean_absolute_error(y_train, train_pred)
    mae_results[name] = mae

# MAE ì¶œë ¥
print("ëª¨ë¸ë³„ MAE ê²°ê³¼:")
for name, score in mae_results.items():
    print(f"{name}: {score:.2f}")

# ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì„ íƒ
best_model_name = min(mae_results, key=mae_results.get)
print(f"\nâœ… ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸: {best_model_name}")

# í•´ë‹¹ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
test_df['ì˜ˆì¸¡_ì „ê¸°ìš”ê¸ˆ(ì›)'] = final_predictions[best_model_name]
submission = test_df[['id', 'ì˜ˆì¸¡_ì „ê¸°ìš”ê¸ˆ(ì›)']].copy()
submission.columns = ['id', 'ì „ê¸°ìš”ê¸ˆ(ì›)']
submission.to_csv("./data/submission_best_model.csv", index=False)
print("\nğŸ“ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: ./data/submission_best_model.csv")


submission.head()
submission.info()
a = pd.read_csv("./data/test.csv")

a.info()

b = pd.read_csv("./submission.csv")
b.info()

c = pd.read_csv("./data/submission_best_model.csv")
c.info()


import pandas as pd

# ë°ì´í„° ë¡œë“œ
train = pd.read_csv("./data/train.csv")
test = pd.read_csv("./data/test.csv")
weather = pd.read_csv("./data/weather.csv")
train.head()
weather.head()
# datetime ë³€í™˜
train['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(train['ì¸¡ì •ì¼ì‹œ'])
test['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(test['ì¸¡ì •ì¼ì‹œ'])
weather['ì¼ì‹œ'] = pd.to_datetime(weather['ì¼ì‹œ'], errors='coerce')

# â±ï¸ ì •í™•íˆ 15ë¶„ ê°„ê²©ìœ¼ë¡œ ë°˜ì˜¬ë¦¼ (round) ì²˜ë¦¬
weather['ì¸¡ì •ì¼ì‹œ'] = weather['ì¼ì‹œ'].dt.round('15T')

# ğŸ§¹ í˜¹ì‹œ ëª¨ë¥¼ ê²°ì¸¡ì¹˜ ì œê±°
weather = weather.dropna(subset=['ì¸¡ì •ì¼ì‹œ', 'ê¸°ì˜¨(Â°C)', 'ìŠµë„(%)'])

# í‰ê·  ì§‘ê³„
weather_grouped = weather.groupby('ì¸¡ì •ì¼ì‹œ')[['ê¸°ì˜¨(Â°C)', 'ìŠµë„(%)']].mean().reset_index()
weather_grouped = weather_grouped.rename(columns={'ê¸°ì˜¨(Â°C)': 'ê¸°ì˜¨', 'ìŠµë„(%)': 'ìŠµë„'})

# ë³‘í•©
train_merged = pd.merge(train, weather_grouped, on='ì¸¡ì •ì¼ì‹œ', how='left')
test_merged = pd.merge(test, weather_grouped, on='ì¸¡ì •ì¼ì‹œ', how='left')

# ê²°ì¸¡ ë³´ê°„ (ì˜µì…˜)
train_merged[['ê¸°ì˜¨', 'ìŠµë„']] = train_merged[['ê¸°ì˜¨', 'ìŠµë„']].fillna(method='ffill').fillna(method='bfill')
test_merged[['ê¸°ì˜¨', 'ìŠµë„']] = test_merged[['ê¸°ì˜¨', 'ìŠµë„']].fillna(method='ffill').fillna(method='bfill')

# ì €ì¥
train_merged.to_csv("./data/train_with_weather.csv", index=False)
test_merged.to_csv("./data/test_with_weather.csv", index=False)

# í™•ì¸
print(train_merged[['ì¸¡ì •ì¼ì‹œ', 'ê¸°ì˜¨', 'ìŠµë„']].head(10))
print("\nâœ… ë³‘í•© ì™„ë£Œ")

train_merged.info()
train_merged.columns


import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from datetime import timedelta

# ë°ì´í„° ë¡œë”©
df = pd.read_csv("./data/train_with_weather.csv")

# datetimeìœ¼ë¡œ ë³€í™˜ ë° ì¸ë±ìŠ¤ ì„¤ì •
df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(df['ì¸¡ì •ì¼ì‹œ'])
df.set_index('ì¸¡ì •ì¼ì‹œ', inplace=True)

# ìŠ¤ì¼€ì¼ë§ ëŒ€ìƒ ìˆ˜ì¹˜í˜• ì—´ë§Œ ì„ íƒ
target_cols = ['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)',
               'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)', 'ê¸°ì˜¨', 'ìŠµë„']

scaler = MinMaxScaler()
scaled = scaler.fit_transform(df[target_cols])
scaled_df = pd.DataFrame(scaled, index=df.index, columns=target_cols)

# í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° êµ¬ì„±
X_train, y_train, X_test, y_test, test_timestamps = [], [], [], [], []

for current_time in scaled_df.index:
    if current_time.month == 9:
        past_times = [current_time - timedelta(weeks=w) for w in range(8, 0, -1)]
        if all(t in scaled_df.index for t in past_times):
            X_test.append(scaled_df.loc[past_times].values)
            y_test.append(scaled_df.loc[current_time].values)
            test_timestamps.append(current_time)
    elif current_time.month < 9:
        past_times = [current_time - timedelta(weeks=w) for w in range(8, 0, -1)]
        if all(t in scaled_df.index for t in past_times):
            X_train.append(scaled_df.loc[past_times].values)
            y_train.append(scaled_df.loc[current_time].values)

X_train, y_train = np.array(X_train), np.array(y_train)
X_test, y_test = np.array(X_test), np.array(y_test)
test_timestamps = np.array(test_timestamps)

print(X_train.shape, X_test.shape)


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# Build an enhanced LSTM model with regularization and normalization
model = Sequential([
    LSTM(128, return_sequences=False, input_shape=(8, 8)),
    BatchNormalization(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(8)
])

model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Early stopping to prevent overfitting
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, 
                    epochs=50, 
                    batch_size=32, 
                    validation_split=0.1, 
                    callbacks=[early_stop],
                    verbose=0)

# Predict on September test set
y_pred_scaled = model.predict(X_test)

# Inverse transform predictions and targets
y_pred = scaler.inverse_transform(y_pred_scaled)
y_true = scaler.inverse_transform(y_test)

# Create DataFrame for comparison
# ìˆ˜ì¹˜í˜• target ì»¬ëŸ¼ë§Œ ëª…ì‹œì ìœ¼ë¡œ ì‚¬ìš©
target_cols = ['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)',
               'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)', 'ê¸°ì˜¨', 'ìŠµë„']

# ìˆ˜ì •ëœ ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„±
results_df = pd.DataFrame(y_true, columns=target_cols)
results_df['ì¸¡ì •ì¼ì‹œ'] = test_timestamps
results_df = results_df.set_index('ì¸¡ì •ì¼ì‹œ')

pred_df = pd.DataFrame(y_pred, columns=[col + '_ì˜ˆì¸¡' for col in target_cols])
pred_df['ì¸¡ì •ì¼ì‹œ'] = test_timestamps
pred_df = pred_df.set_index('ì¸¡ì •ì¼ì‹œ')

# ê²°ê³¼ ë³‘í•©
full_result = pd.concat([results_df, pred_df], axis=1)

import ace_tools as tools 
tools.display_dataframe_to_user(name="LSTM 9ì›” ì˜ˆì¸¡ ê²°ê³¼ ë¹„êµ", dataframe=full_result)


# Full corrected version of single-target LSTM per variable with proper training/test split
import pandas as pd
import numpy as np
from datetime import timedelta
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
import os

# Load and preprocess data
df = pd.read_csv("./data/train_with_weather.csv", encoding="utf-8")
df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(df['ì¸¡ì •ì¼ì‹œ'])
df.set_index('ì¸¡ì •ì¼ì‹œ', inplace=True)

# Target variables
target_cols = ['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)',
               'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)', 'ê¸°ì˜¨', 'ìŠµë„']

# Normalize
scaler = MinMaxScaler()
scaled_all = scaler.fit_transform(df[target_cols])
data_scaled = pd.DataFrame(scaled_all, columns=target_cols, index=df.index)

# Define sequence length (7 days of 15-minute intervals)
seq_len = 96 * 7

# Prepare combined result container
results_combined = []

# Iterate over each variable
for target in target_cols:
    X, y, timestamps = [], [], []

    for i in range(seq_len, len(data_scaled) - 1):
        input_seq = data_scaled.iloc[i - seq_len:i].values
        target_value = data_scaled[target].iloc[i + 1]
        timestamp = data_scaled.index[i + 1]

        X.append(input_seq)
        y.append(target_value)
        timestamps.append(timestamp)

    X = np.array(X)
    y = np.array(y)
    timestamps = np.array(timestamps)

    # Split: before Sep = train, Sep = test
    train_idx = [i for i, t in enumerate(timestamps) if t < pd.Timestamp("2024-09-01")]
    test_idx = [i for i, t in enumerate(timestamps) if pd.Timestamp("2024-09-01") <= t < pd.Timestamp("2024-10-01")]

    X_train, y_train = X[train_idx], y[train_idx]
    X_test, y_test = X[test_idx], y[test_idx]
    test_timestamps = timestamps[test_idx]

    if len(X_train) < 10 or len(X_test) == 0:
        print(f"âŒ {target}: í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    # Build LSTM model
    model = Sequential([
        LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    # Train with conditional validation
    if len(X_train) < 100:
        model.fit(X_train, y_train, epochs=30, batch_size=8, callbacks=[early_stop], verbose=1)
    else:
        model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stop], verbose=1)

    # Predict and inverse transform
    y_pred_scaled = model.predict(X_test).flatten().reshape(-1, 1)
    y_test_scaled = y_test.reshape(-1, 1)

    target_scaler = MinMaxScaler()
    target_scaler.min_, target_scaler.scale_ = scaler.min_[target_cols.index(target)], scaler.scale_[target_cols.index(target)]
    y_pred = y_pred_scaled * target_scaler.scale_ + target_scaler.min_
    y_true = y_test_scaled * target_scaler.scale_ + target_scaler.min_

    # Store results
    temp_df = pd.DataFrame({
        'ì¸¡ì •ì¼ì‹œ': test_timestamps,
        target: y_true.flatten(),
        f"{target}_ì˜ˆì¸¡": y_pred.flatten()
    })
    results_combined.append(temp_df)

# Merge all results
final_result = results_combined[0]
for df_part in results_combined[1:]:
    final_result = pd.merge(final_result, df_part, on='ì¸¡ì •ì¼ì‹œ')

final_result.set_index('ì¸¡ì •ì¼ì‹œ', inplace=True)

# Save to CSV
os.makedirs("./data", exist_ok=True)
output_path = "./data/LSTM_ê°œë³„ëª¨ë¸_ì˜ˆì¸¡_vs_ì‹¤ì œ.csv"
final_result.to_csv(output_path, encoding="utf-8-sig")

output_path


import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# 1. ì›ë³¸ í•™ìŠµ ë°ì´í„°ì—ì„œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
train_df = pd.read_csv("./data/train_with_weather.csv", encoding="utf-8")
target_cols = ['ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)', 'ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)', 'ì§„ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)',
               'íƒ„ì†Œë°°ì¶œëŸ‰(tCO2)', 'ì§€ìƒì—­ë¥ (%)', 'ì§„ìƒì—­ë¥ (%)', 'ê¸°ì˜¨', 'ìŠµë„']

scaler = MinMaxScaler()
scaler.fit(train_df[target_cols])

# 2. ì˜ˆì¸¡ ê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°
scaled_df = pd.read_csv("./data/LSTM_ê°œë³„ëª¨ë¸_ì˜ˆì¸¡_vs_ì‹¤ì œ.csv", encoding="utf-8-sig")

# 3. ìŠ¤ì¼€ì¼ëœ ê°’ë“¤ë§Œ ë”°ë¡œ ì¶”ì¶œ
scaled_only = scaled_df[[col for col in scaled_df.columns if col in target_cols or col.endswith("_ì˜ˆì¸¡")]]

# 4. ì—­ë³€í™˜ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ì¬ì •ë ¬
# ì˜ˆ: ["ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)", "ì „ë ¥ì‚¬ìš©ëŸ‰(kWh)_ì˜ˆì¸¡", "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)", "ì§€ìƒë¬´íš¨ì „ë ¥ëŸ‰(kVarh)_ì˜ˆì¸¡", ...]
recovered_parts = []
for col in target_cols:
    pair = scaled_df[[col, f"{col}_ì˜ˆì¸¡"]].copy()
    scaler_col = MinMaxScaler()
    scaler_col.fit(train_df[[col]])  # ê°œë³„ ì»¬ëŸ¼ë§Œ í•™ìŠµ
    pair[[col, f"{col}_ì˜ˆì¸¡"]] = scaler_col.inverse_transform(pair[[col, f"{col}_ì˜ˆì¸¡"]])
    recovered_parts.append(pair)

# 5. ì»¬ëŸ¼ í†µí•©
restored_df = pd.concat([scaled_df['ì¸¡ì •ì¼ì‹œ']] + recovered_parts, axis=1)
restored_df.to_csv("./data/LSTM_ì˜ˆì¸¡_ë³µêµ¬_ì •ìƒ.csv", index=False, encoding="utf-8-sig")
print("âœ… ë³µì› ì™„ë£Œ: LSTM_ì˜ˆì¸¡_ë³µêµ¬_ì •ìƒ.csv")








