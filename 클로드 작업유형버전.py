
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor  # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

print("ğŸ“Š ì „ê¸°ìš”ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸ - ì„±ëŠ¥ ê°œì„  ë²„ì „")
print("=" * 50)

# ğŸ“‚ ê²½ë¡œ ì„¤ì •
BASE_DIR = "./data"
train_path = os.path.join(BASE_DIR, "train.csv")
test_path = os.path.join(BASE_DIR, "test.csv")

# ================================
# ğŸ“Š 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# ================================
print("1ï¸âƒ£ ë°ì´í„° ë¡œë”© ì¤‘...")
train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)

print(f"   - í›ˆë ¨ ë°ì´í„°: {train_df.shape}")
print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_df.shape}")

# ================================
# ğŸ•’ 2. datetime íŒŒì‹± + ê¸°ë³¸ ì‹œê³„ì—´ íŒŒìƒë³€ìˆ˜
# ================================
print("2ï¸âƒ£ ì‹œê³„ì—´ í”¼ì²˜ ìƒì„± ì¤‘...")
for df in [train_df, test_df]:
    df['ì¸¡ì •ì¼ì‹œ'] = pd.to_datetime(df['ì¸¡ì •ì¼ì‹œ'])
    df['ì›”'] = df['ì¸¡ì •ì¼ì‹œ'].dt.month
    df['ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.day
    df['ì‹œê°„'] = df['ì¸¡ì •ì¼ì‹œ'].dt.hour
    df['ìš”ì¼'] = df['ì¸¡ì •ì¼ì‹œ'].dt.weekday  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
    
    # ğŸ­ ê³µì¥ íŠ¹ì„± ë°˜ì˜: ì¼ìš”ì¼(6), ì›”ìš”ì¼(0)ì— ì „ê¸° ëœ ì‚¬ìš©
    df['ê³µì¥íœ´ë¬´ì¼'] = ((df['ìš”ì¼'] == 0) | (df['ìš”ì¼'] == 6)).astype(int)
    df['í‰ì¼ì—¬ë¶€'] = (df['ìš”ì¼'].between(1, 5)).astype(int)  # í™”~í† ìš”ì¼
    
    # ğŸ“… ë‚ ì§œ ê´€ë ¨ í”¼ì²˜
    df['ì›”ë§ì—¬ë¶€'] = (df['ì¼'] >= 28).astype(int)
    df['ì›”ì´ˆì—¬ë¶€'] = (df['ì¼'] <= 3).astype(int)
    df['ì›”ì¤‘ìˆœì—¬ë¶€'] = (df['ì¼'].between(11, 20)).astype(int)
    
    # ğŸŒ€ ì£¼ê¸°ì„± í”¼ì²˜ (sin/cos ë³€í™˜)
    df['sin_ì‹œê°„'] = np.sin(2 * np.pi * df['ì‹œê°„'] / 24)
    df['cos_ì‹œê°„'] = np.cos(2 * np.pi * df['ì‹œê°„'] / 24)
    df['sin_ì›”'] = np.sin(2 * np.pi * df['ì›”'] / 12)
    df['cos_ì›”'] = np.cos(2 * np.pi * df['ì›”'] / 12)
    df['sin_ì¼'] = np.sin(2 * np.pi * df['ì¼'] / 31)
    df['cos_ì¼'] = np.cos(2 * np.pi * df['ì¼'] / 31)
    df['sin_ìš”ì¼'] = np.sin(2 * np.pi * df['ìš”ì¼'] / 7)
    df['cos_ìš”ì¼'] = np.cos(2 * np.pi * df['ìš”ì¼'] / 7)

# ================================
# ğŸŒ 3. ê³„ì ˆ/ì‹œê°„ëŒ€ êµ¬ë¶„ + ìš”ê¸ˆë‹¨ê°€ ê³„ì‚°
# ================================
print("3ï¸âƒ£ ê³„ì ˆë³„ ìš”ê¸ˆ ì²´ê³„ ë° ì‘ì—…ìœ í˜•ë³„ ë¶€í•˜ ë§¤í•‘ ì¤‘...")

def get_season(month):
    """ê³„ì ˆ êµ¬ë¶„ í•¨ìˆ˜"""
    if month in [6, 7, 8]:
        return 'ì—¬ë¦„'
    elif month in [3, 4, 5, 9, 10]:
        return 'ë´„ê°€ì„'
    else:
        return 'ê²¨ìš¸'

def map_work_type_to_load(work_type):
    """ì‘ì—…ìœ í˜•ì„ ë¶€í•˜ êµ¬ë¶„ìœ¼ë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜"""
    load_mapping = {
        'Light_Load': 'ê²½ë¶€í•˜',
        'Medium_Load': 'ì¤‘ê°„ë¶€í•˜', 
        'Maximum_Load': 'ìµœëŒ€ë¶€í•˜'
    }
    return load_mapping.get(work_type, 'ê²½ë¶€í•˜')  # ê¸°ë³¸ê°’ì€ ê²½ë¶€í•˜

# ğŸ“Š ìš”ê¸ˆ ë‹¨ê°€ í…Œì´ë¸”
rate_table = {
    'before': {
        'ì—¬ë¦„': {'ê²½ë¶€í•˜': 93.1, 'ì¤‘ê°„ë¶€í•˜': 146.3, 'ìµœëŒ€ë¶€í•˜': 216.6},
        'ë´„ê°€ì„': {'ê²½ë¶€í•˜': 93.1, 'ì¤‘ê°„ë¶€í•˜': 115.2, 'ìµœëŒ€ë¶€í•˜': 138.9},
        'ê²¨ìš¸': {'ê²½ë¶€í•˜': 100.4, 'ì¤‘ê°„ë¶€í•˜': 146.5, 'ìµœëŒ€ë¶€í•˜': 193.4}
    },
    'after': {
        'ì—¬ë¦„': {'ê²½ë¶€í•˜': 110.0, 'ì¤‘ê°„ë¶€í•˜': 163.2, 'ìµœëŒ€ë¶€í•˜': 233.5},
        'ë´„ê°€ì„': {'ê²½ë¶€í•˜': 110.0, 'ì¤‘ê°„ë¶€í•˜': 132.1, 'ìµœëŒ€ë¶€í•˜': 155.8},
        'ê²¨ìš¸': {'ê²½ë¶€í•˜': 117.3, 'ì¤‘ê°„ë¶€í•˜': 163.4, 'ìµœëŒ€ë¶€í•˜': 210.3}
    }
}

cutoff_date = datetime(2024, 10, 24)

for df in [train_df, test_df]:
    df['ê³„ì ˆ'] = df['ì›”'].apply(get_season)
    df['ì ìš©ì‹œì '] = df['ì¸¡ì •ì¼ì‹œ'].apply(lambda x: 'before' if x < cutoff_date else 'after')
    # ì‘ì—…ìœ í˜•ìœ¼ë¡œë¶€í„° ì‹œê°„ëŒ€(ë¶€í•˜êµ¬ë¶„) ë§¤í•‘
    df['ì‹œê°„ëŒ€'] = df['ì‘ì—…ìœ í˜•'].apply(map_work_type_to_load)
    df['ìš”ê¸ˆë‹¨ê°€'] = df.apply(lambda row: rate_table[row['ì ìš©ì‹œì ']][row['ê³„ì ˆ']][row['ì‹œê°„ëŒ€']], axis=1)

# ================================
# ğŸ”¤ 4. ì‘ì—…ìœ í˜• ì¸ì½”ë”© + Target Encoding
# ================================
print("4ï¸âƒ£ ì‘ì—…ìœ í˜• ì¸ì½”ë”© ë° Target Encoding ì¤‘...")

# ê¸°ë³¸ ë¼ë²¨ ì¸ì½”ë”©
le = LabelEncoder()
train_df['ì‘ì—…ìœ í˜•_encoded'] = le.fit_transform(train_df['ì‘ì—…ìœ í˜•'])
test_df['ì‘ì—…ìœ í˜•_encoded'] = le.transform(test_df['ì‘ì—…ìœ í˜•'])

# ğŸ¯ Target Encoding (ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ Smoothing ì ìš©)
def target_encoding_with_smoothing(df_train, df_test, categorical_col, target_col, smoothing=10):
    """Target Encoding with Smoothing"""
    # ì „ì²´ í‰ê· 
    global_mean = df_train[target_col].mean()
    
    # ê·¸ë£¹ë³„ í†µê³„
    group_stats = df_train.groupby(categorical_col)[target_col].agg(['mean', 'count']).reset_index()
    group_stats.columns = [categorical_col, 'target_mean', 'count']
    
    # Smoothing ì ìš©
    group_stats['smoothed_target'] = (
        (group_stats['target_mean'] * group_stats['count'] + global_mean * smoothing) /
        (group_stats['count'] + smoothing)
    )
    
    # ë§¤í•‘
    mapping = dict(zip(group_stats[categorical_col], group_stats['smoothed_target']))
    
    return mapping

# ë‹¤ì–‘í•œ Target Encoding ì ìš©
target_encodings = {}

# ì‘ì—…ìœ í˜•ë³„ Target Encoding
target_encodings['ì‘ì—…ìœ í˜•'] = target_encoding_with_smoothing(train_df, test_df, 'ì‘ì—…ìœ í˜•', 'ì „ê¸°ìš”ê¸ˆ(ì›)')
train_df['ì‘ì—…ìœ í˜•_target'] = train_df['ì‘ì—…ìœ í˜•'].map(target_encodings['ì‘ì—…ìœ í˜•'])
test_df['ì‘ì—…ìœ í˜•_target'] = test_df['ì‘ì—…ìœ í˜•'].map(target_encodings['ì‘ì—…ìœ í˜•'])

# ì‹œê°„ë³„ Target Encoding
target_encodings['ì‹œê°„'] = target_encoding_with_smoothing(train_df, test_df, 'ì‹œê°„', 'ì „ê¸°ìš”ê¸ˆ(ì›)')
train_df['ì‹œê°„_target'] = train_df['ì‹œê°„'].map(target_encodings['ì‹œê°„'])
test_df['ì‹œê°„_target'] = test_df['ì‹œê°„'].map(target_encodings['ì‹œê°„'])

# ìš”ì¼ë³„ Target Encoding
target_encodings['ìš”ì¼'] = target_encoding_with_smoothing(train_df, test_df, 'ìš”ì¼', 'ì „ê¸°ìš”ê¸ˆ(ì›)')
train_df['ìš”ì¼_target'] = train_df['ìš”ì¼'].map(target_encodings['ìš”ì¼'])
test_df['ìš”ì¼_target'] = test_df['ìš”ì¼'].map(target_encodings['ìš”ì¼'])

# ì‹œê°„ëŒ€ë³„ Target Encoding
target_encodings['ì‹œê°„ëŒ€'] = target_encoding_with_smoothing(train_df, test_df, 'ì‹œê°„ëŒ€', 'ì „ê¸°ìš”ê¸ˆ(ì›)')
train_df['ì‹œê°„ëŒ€_target'] = train_df['ì‹œê°„ëŒ€'].map(target_encodings['ì‹œê°„ëŒ€'])
test_df['ì‹œê°„ëŒ€_target'] = test_df['ì‹œê°„ëŒ€'].map(target_encodings['ì‹œê°„ëŒ€'])

# ================================
# ğŸ”„ 5. ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±
# ================================
print("5ï¸âƒ£ ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ì¤‘...")
# LabelEncoder ì¸ì½”ë”©
le_season = LabelEncoder()
train_df['ê³„ì ˆ_encoded'] = le_season.fit_transform(train_df['ê³„ì ˆ'])
test_df['ê³„ì ˆ_encoded'] = le_season.transform(test_df['ê³„ì ˆ'])

# ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„±
for df in [train_df, test_df]:
    df['ë¶€í•˜êµ¬ë¶„_encoded'] = df['ì‘ì—…ìœ í˜•'].map({
        'Light_Load': 0, 'Medium_Load': 1, 'Maximum_Load': 2
    })
    df['ìš”ê¸ˆë‹¨ê°€_ì‘ì—…ìœ í˜•'] = df['ìš”ê¸ˆë‹¨ê°€'] * df['ì‘ì—…ìœ í˜•_encoded']
    df['ìš”ê¸ˆë‹¨ê°€_ì‹œê°„'] = df['ìš”ê¸ˆë‹¨ê°€'] * df['ì‹œê°„']
    df['ë¶€í•˜êµ¬ë¶„_ì‹œê°„'] = df['ë¶€í•˜êµ¬ë¶„_encoded'] * df['ì‹œê°„']
    df['ë¶€í•˜êµ¬ë¶„_ê³µì¥íœ´ë¬´ì¼'] = df['ë¶€í•˜êµ¬ë¶„_encoded'] * df['ê³µì¥íœ´ë¬´ì¼']
    df['ê³„ì ˆ_ë¶€í•˜êµ¬ë¶„'] = df['ê³„ì ˆ_encoded'] * df['ë¶€í•˜êµ¬ë¶„_encoded']

# ================================
# ğŸ§¹ 6. ì´ìƒì¹˜ ì²˜ë¦¬
# ================================
print("6ï¸âƒ£ ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘...")

# IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
Q1 = train_df['ì „ê¸°ìš”ê¸ˆ(ì›)'].quantile(0.25)
Q3 = train_df['ì „ê¸°ìš”ê¸ˆ(ì›)'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print(f"   - ì´ìƒì¹˜ ë²”ìœ„: {lower_bound:.2f} ~ {upper_bound:.2f}")
outliers_count = len(train_df[(train_df['ì „ê¸°ìš”ê¸ˆ(ì›)'] < lower_bound) | (train_df['ì „ê¸°ìš”ê¸ˆ(ì›)'] > upper_bound)])
print(f"   - ì´ìƒì¹˜ ê°œìˆ˜: {outliers_count}")

# ì´ìƒì¹˜ ì œê±° (ë„ˆë¬´ ê·¹ë‹¨ì ì¸ ê°’ë§Œ ì œê±°)
train_df_clean = train_df[(train_df['ì „ê¸°ìš”ê¸ˆ(ì›)'] >= lower_bound) & (train_df['ì „ê¸°ìš”ê¸ˆ(ì›)'] <= upper_bound)].copy()
print(f"   - ì •ì œ í›„ í›ˆë ¨ ë°ì´í„°: {train_df_clean.shape[0]} (ì œê±°: {len(train_df) - len(train_df_clean)}ê°œ)")

# ================================
# ğŸ¯ 7. í”¼ì²˜ ì„ íƒ ë° ë°ì´í„° ì¤€ë¹„
# ================================
print("7ï¸âƒ£ í”¼ì²˜ ì„ íƒ ë° ë°ì´í„° ì¤€ë¹„ ì¤‘...")

# ì‚¬ìš©í•  í”¼ì²˜ ì •ì˜
features = [
    # ê¸°ë³¸ í”¼ì²˜
    'ì‘ì—…ìœ í˜•_encoded', 'ë¶€í•˜êµ¬ë¶„_encoded', 'ì›”', 'ì¼', 'ì‹œê°„', 'ìš”ì¼',
    
    # ê³µì¥ íŠ¹ì„± í”¼ì²˜
    'ê³µì¥íœ´ë¬´ì¼', 'í‰ì¼ì—¬ë¶€',
    
    # ë‚ ì§œ ê´€ë ¨ í”¼ì²˜
    'ì›”ë§ì—¬ë¶€', 'ì›”ì´ˆì—¬ë¶€', 'ì›”ì¤‘ìˆœì—¬ë¶€',
    
    # ì£¼ê¸°ì„± í”¼ì²˜
    'sin_ì‹œê°„', 'cos_ì‹œê°„', 'sin_ì›”', 'cos_ì›”', 'sin_ìš”ì¼', 'cos_ìš”ì¼',
    
    # ìš”ê¸ˆ ê´€ë ¨ í”¼ì²˜
    'ìš”ê¸ˆë‹¨ê°€', 'ê³„ì ˆ_encoded',
    
    # Target Encoding í”¼ì²˜
    'ì‘ì—…ìœ í˜•_target', 'ì‹œê°„_target', 'ìš”ì¼_target', 'ì‹œê°„ëŒ€_target',
    
    # ìƒí˜¸ì‘ìš© í”¼ì²˜
    'ìš”ê¸ˆë‹¨ê°€_ì‘ì—…ìœ í˜•', 'ìš”ê¸ˆë‹¨ê°€_ì‹œê°„', 
    'ê³µì¥íœ´ë¬´ì¼_ì‘ì—…ìœ í˜•', 'ê³µì¥íœ´ë¬´ì¼_ì‹œê°„',
    'ë¶€í•˜êµ¬ë¶„_ì‹œê°„', 'ë¶€í•˜êµ¬ë¶„_ê³µì¥íœ´ë¬´ì¼', 'ê³„ì ˆ_ë¶€í•˜êµ¬ë¶„',
    
    # ì‹œê°„ëŒ€ í”¼ì²˜
    'ì˜¤ì „ì‹œê°„', 'ì˜¤í›„ì‹œê°„', 'ì €ë…ì‹œê°„', 'ìƒˆë²½ì‹œê°„'
]

target = 'ì „ê¸°ìš”ê¸ˆ(ì›)'

# í”¼ì²˜ ì¡´ì¬ í™•ì¸
missing_features = [f for f in features if f not in train_df_clean.columns]
if missing_features:
    print(f"   âš ï¸  ëˆ„ë½ëœ í”¼ì²˜: {missing_features}")
    features = [f for f in features if f in train_df_clean.columns]

print(f"   - ì‚¬ìš©í•  í”¼ì²˜ ê°œìˆ˜: {len(features)}")

# ë°ì´í„° ë¶„í• 
X = train_df_clean[features]
y = train_df_clean[target]
X_test = test_df[features]

# ìŠ¤ì¼€ì¼ë§ (RobustScaler ì‚¬ìš© - ì´ìƒì¹˜ì— ëœ ë¯¼ê°)
print("8ï¸âƒ£ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì¤‘...")
scaler = RobustScaler()

# ì—°ì†í˜• ë³€ìˆ˜ë§Œ ìŠ¤ì¼€ì¼ë§
numeric_features = ['ìš”ê¸ˆë‹¨ê°€', 'ì‘ì—…ìœ í˜•_target', 'ì‹œê°„_target', 'ìš”ì¼_target', 'ì‹œê°„ëŒ€_target']
numeric_features = [f for f in numeric_features if f in features]

if numeric_features:
    X_scaled = X.copy()
    X_test_scaled = X_test.copy()
    
    X_scaled[numeric_features] = scaler.fit_transform(X[numeric_features])
    X_test_scaled[numeric_features] = scaler.transform(X_test[numeric_features])
else:
    X_scaled = X
    X_test_scaled = X_test

# í›ˆë ¨/ê²€ì¦ ë¶„í• 
X_train, X_val, y_train, y_val = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"   - í›ˆë ¨ ì„¸íŠ¸: {X_train.shape}")
print(f"   - ê²€ì¦ ì„¸íŠ¸: {X_val.shape}")

# ================================
# ğŸ¤– 8. ëª¨ë¸ í›ˆë ¨ (ì•™ìƒë¸”)
# ================================
print("9ï¸âƒ£ ëª¨ë¸ í›ˆë ¨ ì¤‘...")

# ì—¬ëŸ¬ ëª¨ë¸ ì •ì˜
models = {
    'xgb': XGBRegressor(
        n_estimators=500,
        max_depth=6,
        learning_rate=0.02,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1
    ),
    'lgb': LGBMRegressor(
        n_estimators=500,
        max_depth=6,
        learning_rate=0.02,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=-1,
        verbose=-1
    ),
    'rf': RandomForestRegressor(
        n_estimators=300,
        max_depth=10,
        random_state=42,
        n_jobs=-1
    )
}

# ëª¨ë¸ë³„ í›ˆë ¨ ë° ê²€ì¦
model_predictions = {}
model_scores = {}

for name, model in models.items():
    print(f"   - {name.upper()} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
    
    # ëª¨ë¸ í›ˆë ¨
    model.fit(X_train, y_train)
    
    # ê²€ì¦ ì˜ˆì¸¡
    val_pred = model.predict(X_val)
    
    # ì„±ëŠ¥ í‰ê°€
    mae = mean_absolute_error(y_val, val_pred)
    rmse = np.sqrt(mean_squared_error(y_val, val_pred))
    r2 = r2_score(y_val, val_pred)
    
    model_scores[name] = {'MAE': mae, 'RMSE': rmse, 'R2': r2}
    
    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
    test_pred = model.predict(X_test_scaled)
    model_predictions[name] = test_pred
    
    print(f"     MAE: {mae:.2f}, RMSE: {rmse:.2f}, RÂ²: {r2:.4f}")

# ================================
# ğŸ¯ 9. ì•™ìƒë¸” ì˜ˆì¸¡
# ================================
print("ğŸ”Ÿ ì•™ìƒë¸” ì˜ˆì¸¡ ì¤‘...")

# ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘í‰ê·  (R2 ìŠ¤ì½”ì–´ ê¸°ë°˜)
weights = {}
total_r2 = sum([scores['R2'] for scores in model_scores.values()])

for name, scores in model_scores.items():
    weights[name] = scores['R2'] / total_r2

print("   - ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜:")
for name, weight in weights.items():
    print(f"     {name.upper()}: {weight:.3f}")

# ê°€ì¤‘í‰ê·  ì•™ìƒë¸”
ensemble_pred = np.zeros(len(X_test_scaled))
for name, pred in model_predictions.items():
    ensemble_pred += weights[name] * pred

# ì•™ìƒë¸” ê²€ì¦ ì„±ëŠ¥ ê³„ì‚°
ensemble_val_pred = np.zeros(len(X_val))
for name, model in models.items():
    val_pred = model.predict(X_val)
    ensemble_val_pred += weights[name] * val_pred

ensemble_mae = mean_absolute_error(y_val, ensemble_val_pred)
ensemble_rmse = np.sqrt(mean_squared_error(y_val, ensemble_val_pred))
ensemble_r2 = r2_score(y_val, ensemble_val_pred)

print("\nğŸ“Š ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥:")
print(f"   MAE: {ensemble_mae:.2f}")
print(f"   RMSE: {ensemble_rmse:.2f}")
print(f"   RÂ²: {ensemble_r2:.4f}")

# ================================
# ğŸ’¾ 10. ê²°ê³¼ ì €ì¥
# ================================
print("1ï¸âƒ£1ï¸âƒ£ ê²°ê³¼ ì €ì¥ ì¤‘...")

# ì œì¶œ íŒŒì¼ ìƒì„±
test_df['ì „ê¸°ìš”ê¸ˆ(ì›)'] = ensemble_pred
submission = test_df[['id', 'ì „ê¸°ìš”ê¸ˆ(ì›)']].copy()

# ìŒìˆ˜ ê°’ ì²˜ë¦¬ (ì „ê¸°ìš”ê¸ˆì€ ìŒìˆ˜ê°€ ë  ìˆ˜ ì—†ìŒ)
submission['ì „ê¸°ìš”ê¸ˆ(ì›)'] = np.maximum(submission['ì „ê¸°ìš”ê¸ˆ(ì›)'], 0)

# íŒŒì¼ ì €ì¥
submission.to_csv("submission_improved.csv", index=False)

print(f"   - ì œì¶œ íŒŒì¼ ì €ì¥: submission_improved.csv")
print(f"   - ì˜ˆì¸¡ê°’ ë²”ìœ„: {submission['ì „ê¸°ìš”ê¸ˆ(ì›)'].min():.2f} ~ {submission['ì „ê¸°ìš”ê¸ˆ(ì›)'].max():.2f}")

# ================================
# ğŸ“ˆ 11. í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
# ================================
print("1ï¸âƒ£2ï¸âƒ£ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„...")

# XGBoost ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„
feature_importance = pd.DataFrame({
    'feature': features,
    'importance': models['xgb'].feature_importances_
}).sort_values('importance', ascending=False)

print("\nğŸ” ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì²˜:")
for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):
    print(f"   {i+1:2d}. {row['feature']:20s}: {row['importance']:.4f}")

print("\nâœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print("ğŸ“ 'submission_improved.csv' íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")